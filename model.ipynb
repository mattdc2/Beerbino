{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w96GilfNlAH0"
      },
      "source": [
        "#### Transformation des dossiers de stockage des images pour faire apparaitre leur label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6CS2CeIlMBY",
        "outputId": "1d53a16a-7947-41c1-cb29-1148f1ec4777"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Y52d63lKlAH7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/Centrale/Beerbino"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu_kUA46lXxq",
        "outputId": "25356f2d-e7ce-4b2f-cc9c-cabcee2d20ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Centrale/Beerbino\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g89IdvIJlAH-",
        "outputId": "88eeaa6f-f153-4cd2-be02-69ca5f7f6c60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              index_scrapping  \\\n",
              "unique_index                    \n",
              "0                           0   \n",
              "1                           1   \n",
              "2                           2   \n",
              "3                           3   \n",
              "4                           4   \n",
              "...                       ...   \n",
              "1458                      737   \n",
              "1459                      738   \n",
              "1460                      739   \n",
              "1461                      740   \n",
              "1462                      741   \n",
              "\n",
              "                                                     link_photo  \\\n",
              "unique_index                                                      \n",
              "0             https://media.auchan.fr/MEDIASTEP80202553_230x...   \n",
              "1             https://media.auchan.fr/MEDIASTEP157309628_230...   \n",
              "2             https://media.auchan.fr/MEDIASTEP66806491_230x...   \n",
              "3             https://media.auchan.fr/MEDIASTEP137932992_230...   \n",
              "4             https://media.auchan.fr/MEDIASTEP157260428_230...   \n",
              "...                                                         ...   \n",
              "1458          https://d33za54wpumlhy.cloudfront.net/eyJidWNr...   \n",
              "1459          https://d33za54wpumlhy.cloudfront.net/eyJidWNr...   \n",
              "1460          https://d33za54wpumlhy.cloudfront.net/eyJidWNr...   \n",
              "1461          https://d33za54wpumlhy.cloudfront.net/eyJidWNr...   \n",
              "1462          https://d33za54wpumlhy.cloudfront.net/eyJidWNr...   \n",
              "\n",
              "                                                     name  \\\n",
              "unique_index                                                \n",
              "0                                Bière blonde 11,8% boîte   \n",
              "1                        Bière blonde bio 7.2% bouteilles   \n",
              "2                        Bière blonde triple 8% bouteille   \n",
              "3                   Bière brassin de Noël 6,5% bouteilles   \n",
              "4                      Bière blonde triple signature 8.5%   \n",
              "...                                                   ...   \n",
              "1458                      The Bruery - Where Is the Lava?   \n",
              "1459          To Øl x Casey - Through the Eyes of Mortals   \n",
              "1460            The Bruery - So Happens It's Tuesday 2021   \n",
              "1461                Brooklyn x Russian River - Refraction   \n",
              "1462                                   Thornbridge Barden   \n",
              "\n",
              "                            brand              type      website   price  \n",
              "unique_index                                                              \n",
              "0                       BELZEBUTH               NaN       Auchan     NaN  \n",
              "1                      LA GOUDALE               NaN       Auchan     NaN  \n",
              "2               SECRET DES MOINES               NaN       Auchan     NaN  \n",
              "3                      GRIMBERGEN               NaN       Auchan     NaN  \n",
              "4                           CH'TI               NaN       Auchan     NaN  \n",
              "...                           ...               ...          ...     ...  \n",
              "1458                   The Bruery  wild / sour beer  SaveurBiere  19,90€  \n",
              "1459                        To Øl  wild / sour beer  SaveurBiere  16,50€  \n",
              "1460                   The Bruery    imperial stout  SaveurBiere  14,30€  \n",
              "1461             Brooklyn Brewery  wild / sour beer  SaveurBiere  24,90€  \n",
              "1462          Thornbridge Brewery  English Pale Ale  SaveurBiere   3,30€  \n",
              "\n",
              "[1463 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f63e8bb-09c2-4b4e-915d-75ce06cbff85\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index_scrapping</th>\n",
              "      <th>link_photo</th>\n",
              "      <th>name</th>\n",
              "      <th>brand</th>\n",
              "      <th>type</th>\n",
              "      <th>website</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique_index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>https://media.auchan.fr/MEDIASTEP80202553_230x...</td>\n",
              "      <td>Bière blonde 11,8% boîte</td>\n",
              "      <td>BELZEBUTH</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Auchan</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>https://media.auchan.fr/MEDIASTEP157309628_230...</td>\n",
              "      <td>Bière blonde bio 7.2% bouteilles</td>\n",
              "      <td>LA GOUDALE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Auchan</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>https://media.auchan.fr/MEDIASTEP66806491_230x...</td>\n",
              "      <td>Bière blonde triple 8% bouteille</td>\n",
              "      <td>SECRET DES MOINES</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Auchan</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>https://media.auchan.fr/MEDIASTEP137932992_230...</td>\n",
              "      <td>Bière brassin de Noël 6,5% bouteilles</td>\n",
              "      <td>GRIMBERGEN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Auchan</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>https://media.auchan.fr/MEDIASTEP157260428_230...</td>\n",
              "      <td>Bière blonde triple signature 8.5%</td>\n",
              "      <td>CH'TI</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Auchan</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1458</th>\n",
              "      <td>737</td>\n",
              "      <td>https://d33za54wpumlhy.cloudfront.net/eyJidWNr...</td>\n",
              "      <td>The Bruery - Where Is the Lava?</td>\n",
              "      <td>The Bruery</td>\n",
              "      <td>wild / sour beer</td>\n",
              "      <td>SaveurBiere</td>\n",
              "      <td>19,90€</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1459</th>\n",
              "      <td>738</td>\n",
              "      <td>https://d33za54wpumlhy.cloudfront.net/eyJidWNr...</td>\n",
              "      <td>To Øl x Casey - Through the Eyes of Mortals</td>\n",
              "      <td>To Øl</td>\n",
              "      <td>wild / sour beer</td>\n",
              "      <td>SaveurBiere</td>\n",
              "      <td>16,50€</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1460</th>\n",
              "      <td>739</td>\n",
              "      <td>https://d33za54wpumlhy.cloudfront.net/eyJidWNr...</td>\n",
              "      <td>The Bruery - So Happens It's Tuesday 2021</td>\n",
              "      <td>The Bruery</td>\n",
              "      <td>imperial stout</td>\n",
              "      <td>SaveurBiere</td>\n",
              "      <td>14,30€</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1461</th>\n",
              "      <td>740</td>\n",
              "      <td>https://d33za54wpumlhy.cloudfront.net/eyJidWNr...</td>\n",
              "      <td>Brooklyn x Russian River - Refraction</td>\n",
              "      <td>Brooklyn Brewery</td>\n",
              "      <td>wild / sour beer</td>\n",
              "      <td>SaveurBiere</td>\n",
              "      <td>24,90€</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1462</th>\n",
              "      <td>741</td>\n",
              "      <td>https://d33za54wpumlhy.cloudfront.net/eyJidWNr...</td>\n",
              "      <td>Thornbridge Barden</td>\n",
              "      <td>Thornbridge Brewery</td>\n",
              "      <td>English Pale Ale</td>\n",
              "      <td>SaveurBiere</td>\n",
              "      <td>3,30€</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1463 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f63e8bb-09c2-4b4e-915d-75ce06cbff85')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1f63e8bb-09c2-4b4e-915d-75ce06cbff85 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1f63e8bb-09c2-4b4e-915d-75ce06cbff85');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "bieres = pd.read_csv(\"scraping/data_beers.csv\",sep=\";\")\n",
        "bieres.columns = ['unique_id','index_scrapping','link_photo','name','brand','type','website','price']\n",
        "bieres.drop(\"unique_id\", inplace=True, axis=1)\n",
        "bieres.rename_axis(index=\"unique_index\", axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cGcZRsZBlAIA"
      },
      "outputs": [],
      "source": [
        "marques = bieres.groupby(\"brand\").all().reset_index()\n",
        "marques.drop([\"index_scrapping\",\"link_photo\",\"name\",\"type\",\"website\",\"price\"], inplace=True, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "x2HfamUMlAIB"
      },
      "outputs": [],
      "source": [
        "list_brands = list(marques.brand)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCc4dn9vlAIC"
      },
      "source": [
        "#### Création des dossiers regroupant les bières par marque\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "A09tCHEWlAID"
      },
      "outputs": [],
      "source": [
        "for brand in list_brands:\n",
        "    try:\n",
        "        os.mkdir(f'scraping/data/folders/{brand}')\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubjqHYOmlAIE"
      },
      "source": [
        "#### Ajout des photos dans les bons dossiers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cKKOOTILlAIF"
      },
      "outputs": [],
      "source": [
        "for image_id in range(1463):\n",
        "    brand = bieres.iloc[image_id].brand\n",
        "    try:\n",
        "        shutil.copyfile(f'scraping/data/images/{image_id}.jpg',f'scraping/data/folders/{brand}/{image_id}.jpg')\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Kv-X5LyWlAIH",
        "outputId": "1f5a33c8-70d0-48ae-abfa-d4e592be0945",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "## Test \n",
        "brand = \"3 MONTS\"\n",
        "len(os.listdir(f'scraping/data/folders/{brand}'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3it5ezplAII"
      },
      "source": [
        "#### Get last id from bieres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O1TStpElAII"
      },
      "outputs": [],
      "source": [
        "bieres.loc[bieres.index.stop] = [\"NaN\",\"NaN\",\"NaN\",brand,\"NaN\",\"NaN\",\"NaN\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-TsI8f0lAIK"
      },
      "source": [
        "### Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_CRYSU5alAIM"
      },
      "outputs": [],
      "source": [
        "brands = os.listdir(f'scraping/data/folders')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "O81lI4qtlAIN"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageFilter\n",
        "import os, sys\n",
        "import random\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "for brand in brands:  \n",
        "    for infile in os.listdir(f'scraping/data/folders/{brand}'):\n",
        "        with Image.open(f'scraping/data/folders/{brand}/{infile}') as im:\n",
        "            try : \n",
        "                bright, satur, contrast, hue, out_90, out_180, out_270, grey, blurred,  = transforms.ColorJitter(brightness=2.5)(im.convert('RGB')), transforms.ColorJitter(saturation=2)(im.convert('RGB')), transforms.ColorJitter(contrast=2)(im.convert('RGB')), transforms.ColorJitter(hue=0.2)(im.convert('RGB')), im.convert('RGB').transpose(Image.ROTATE_90), im.convert('RGB').transpose(Image.ROTATE_180),im.convert('RGB').transpose(Image.ROTATE_270), im.convert('RGB').convert('L'), im.convert('RGB').filter(ImageFilter.BLUR)\n",
        "                new_index = bieres.index[-1] + 1\n",
        "                bright.save(f'scraping/data/folders/{brand}/{new_index}.jpg')\n",
        "                bieres.loc[new_index] = [\"NaN\",\"NaN\",\"NaN\",brand,\"NaN\",\"NaN\",\"NaN\"]\n",
        "                new_index = bieres.index[-1] + 1\n",
        "                satur.save(f'scraping/data/folders/{brand}/{new_index}.jpg')\n",
        "                bieres.loc[new_index] = [\"NaN\",\"NaN\",\"NaN\",brand,\"NaN\",\"NaN\",\"NaN\"]\n",
        "                new_index = bieres.index[-1] + 1\n",
        "                contrast.save(f'scraping/data/folders/{brand}/{new_index}.jpg')\n",
        "                bieres.loc[new_index] = [\"NaN\",\"NaN\",\"NaN\",brand,\"NaN\",\"NaN\",\"NaN\"]\n",
        "                new_index = bieres.index[-1] + 1\n",
        "                hue.save(f'scraping/data/folders/{brand}/{new_index}.jpg')\n",
        "                bieres.loc[new_index] = [\"NaN\",\"NaN\",\"NaN\",brand,\"NaN\",\"NaN\",\"NaN\"]              \n",
        "                new_index = bieres.index[-1] + 1\n",
        "                out_90.save(f'scraping/data/folders/{brand}/{new_index}.jpg')\n",
        "                bieres.loc[new_index] = [\"NaN\",\"NaN\",\"NaN\",brand,\"NaN\",\"NaN\",\"NaN\"]\n",
        "                new_index = bieres.index[-1]+1\n",
        "                out_180.save(f'scraping/data/folders/{brand}/{new_index}.jpg')\n",
        "                bieres.loc[new_index] = [\"NaN\",\"NaN\",\"NaN\",brand,\"NaN\",\"NaN\",\"NaN\"]\n",
        "                new_index = bieres.index[-1]+1\n",
        "                out_270.save(f'scraping/data/folders/{brand}/{new_index}.jpg')\n",
        "                bieres.loc[new_index] = [\"NaN\",\"NaN\",\"NaN\",brand,\"NaN\",\"NaN\",\"NaN\"]\n",
        "                new_index = bieres.index[-1]+1 \n",
        "                grey.save(f'scraping/data/folders/{brand}/{new_index}.jpg')\n",
        "                bieres.loc[new_index] = [\"NaN\",\"NaN\",\"NaN\",brand,\"NaN\",\"NaN\",\"NaN\"]\n",
        "                new_index = bieres.index[-1]+1 \n",
        "                blurred.save(f'scraping/data/folders/{brand}/{new_index}.jpg')\n",
        "                bieres.loc[new_index] = [\"NaN\",\"NaN\",\"NaN\",brand,\"NaN\",\"NaN\",\"NaN\"]\n",
        "            except :\n",
        "                pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove weaker folders"
      ],
      "metadata": {
        "id": "E0nJiaMzUA4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "count=0\n",
        "#brand_removed = []\n",
        "for brand in os.listdir(f'scraping/data/folders'):\n",
        "    nb_bieres_dossier = len(os.listdir(f'scraping/data/folders/{brand}'))\n",
        "    if nb_bieres_dossier < 6 and count < 22 :\n",
        "        shutil.rmtree(f'scraping/data/folders/{brand}')\n",
        "        brand_removed.append(brand)\n",
        "        count+=1\n",
        "print(count, brand_removed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQulR82BUDOD",
        "outputId": "ecf39b19-7aa1-491c-d8b3-c11727213a86"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22 ['1664', '3 Fonteinen', '4 Noses', \"Abbaye d'Achel\", \"Abbaye d'Orval\", 'Abbaye de Grimbergen', 'Abbaye de Maredsous', 'Abbaye de Rochefort', 'Abbaye de Stift Engelszell', 'Abbaye de Westmalle', 'Abbazia delle Tre Fontane', 'Abnormal Beer Co.', 'Alken-Maes', 'Amager Bryghus', 'Anchor Brewing Company', 'Ardwen', 'Asahi Brewers', 'Azimut', 'BELLEROSE', 'BEWIZ', 'BIERE DU COMTE', 'BIERE DU CORBEAU', 'BIERE DU DEMON', 'BLANCHE DE NAMUR', 'BRASSERIE DES 3 PHARES', 'BRASSERIE DU PEPERE', 'BRASSEURS SAVOYARDS', 'BRUNEHAUT', 'BUSH', 'Belle-Vue', 'Big Drop Brewing Co', 'Big Mountain', 'Birra Del Borgo', 'Black Project', 'Boundary', \"Brasserie 't IJ\", 'Brasserie Akerbeltz', 'Brasserie Atrium', 'Brasserie Au Baron', 'Brasserie Beck', 'Brasserie Bockor ', 'Brasserie Castelain', 'Brasserie Caulier', 'Brasserie Coreff', 'Brasserie Corsendonk', 'Brasserie De Hoorn', 'Brasserie De Ranke', 'Brasserie F. Boon', 'Brasserie Gingeur', 'Brasserie Haacht', 'Brasserie Het Anker', 'Brasserie Kulmbacher', 'Brasserie Lefebvre', 'Brasserie Lupulus', 'Brasserie Mélusine', 'Brasserie Pietra', 'Brasserie Saint Germain', 'Brasserie Schloss Eggenberg', 'Brasserie Van Honsebrouck', 'Brasserie Van Steenberge', 'Brasserie Vanuxeem', 'Brasserie Verhaeghe', \"Brasserie d'Achouffe\", \"Brasserie d'Orgemont\", 'Brasserie de Blaugies', 'Brasserie de Brunehaut', 'Brasserie de l Abbaye des Rocs ', 'Brasserie de la Senne', 'Brasserie des légendes', 'Brasserie du Bocq', 'Brasserie du Pays Flamand', 'Brasseries du Maroc', 'Brehon Brewhouse', 'Brew by  Numbers', 'Brique House', 'Brouwerij De Brabandere (Bavik)', 'Brouwerij Kazematten', 'Brouwerij Kees', 'Brouwerij Omer Vander Ghinste', 'Brouwerij Verstraete', 'Budweiser', 'CARLSBERG', 'CAROLUS', 'CASTELAIN', 'CINEY', 'CORBEAU', 'CORNET', 'CORSENDONK', 'CUVEE DES TROLLS', 'Cambier', 'Camden Town Brewery', 'Carlow Brewing Company', 'Cerveceria y Malteria Quilmes', 'Cobra', 'Coconino', 'Collective Arts Brewing', 'Coopers Brewery', 'Coors', 'Corona', 'DE VELDEN', 'DOMINICAINS', 'DUCASSE', 'Diebels', 'Dieu du ciel', 'Dot Brewing', 'Dry & Bitter', 'EKU 28', 'ERDINGER', 'Einstok Olgerd', 'Erdinger', 'Ermitage', 'Extraomnes', 'FOUDROYANTE', 'Fierce', 'Flying Dog Brewery', 'Flügge', 'G DE GOUDALE', 'GALLIA', 'GLENFIDDICH', \"GRAIN D'ORGE\", 'Galibot', 'Garrigues', 'Goose Island Beer Company', 'Gordon', 'Green Flash Brewing Company', \"Guinness St. James's Gate\", 'Gweilo', 'HAPKIN', 'HEINEKEN', 'HOMMELBIER', 'Hacker-Pschorr', 'Hartwall Royal Unibrew', 'Hinano', 'Hofbräuhaus München', 'IBB', 'Iki', 'Innis & Gunn', 'Iron Brewery', 'Jopen', 'Jupiler', 'KEKETTE', 'Kirin Brewery Company', 'Kona Brewing Company', \"L'ANGELUS\", 'LA BIERE DU DEMON', 'LA BOUFONNE', 'LA BÊTE', 'LA CERVOISE DES ANCETRES', 'LA CHOULETTE', 'LA DOCKER', 'LA FIERE', 'LA RAOUL', 'LA TRAPPE', 'LAGUNITAS', 'LE BALLON', 'LE FUT SARLADAIS', 'LEVRETTE', 'LIEFMANS', 'LUPULUS', 'La Brasserie Fondamentale', 'La Cagole', 'La Corne du Bois des Pendus', 'La Pirata Brewing', 'La Virgen', 'La dodo', 'Laugar Brewery', 'Le Détour', 'Liefmans', 'Little Creatures', 'Lowenbräu', 'METEOR', 'MIRA', \"MOULINS D'ASCQ\", 'MOUNTA CALA', 'Malheur', 'McAuslan Brewing', 'Mean Sardine', 'Mikkeller', 'Molson Breweries', 'Mont des Cats', 'Mount St Bernard Abbey', 'NINKASI', 'New Image', 'North Coast', 'Offshoot Beer Co.', 'PAGE 24', 'PAIX DIEU', \"PASTO'RALE\", 'PELFORTH', 'PIRAAT', 'POLAR MONKEYS', 'Paname Brewing Company', 'Paradox', 'Paulaner', 'Poppels Bryggeri', 'PÈRE BRASSEUR', 'ROCHEFORT', 'Rascals Brewing', 'Robinsons', 'Rodenbach', 'SAGRES', 'SLASH', 'ST BERNARDUS', 'ST FEUILLIEN', 'Samuel Adams', 'Sapporo', 'Saveur Bière', 'Sierra Nevada Brewing Company', 'Singha', \"St Joseph's Abbey\", 'Staropramen Breweries', 'Stella Artois', 'TETE DE MULE', 'The Brew Society Heule', 'The Piggy Brewing Company', 'The Rare Barrel', 'The Wild Beer Co.', 'Timmermans', 'Trappistenbrouwerij De Kievit', 'Tsingtao', 'Uiltje Brewing Company', 'VEDETT', 'VICTORIA', 'Val Dieu', 'WEL SCOTCH', 'WESTMALLE', 'Weltenburger', 'Wicked Weed Brewing', 'Yeastie Boys', 'AB InBev', 'Abbaye de Scourmont - Chimay', 'Alpine Beer Company', 'Brasserie De Halve Maan', 'Brasserie Dubuisson', 'Brasserie Franche Montagne (BFM)', 'Brasserie Sainte Crucienne', \"Brasserie de l'Être\", 'DELIRIUM', 'De Dochter van de Korenaar', 'Duvel Moortgat', 'FISCHER', 'GOUDALE', 'GRIMBERGEN', 'GUINNESS', 'Hoegaarden', 'Hoppy Road', 'JADE', 'KWAK', 'MAREDSOUS', 'MORT SUBITE', 'Modern Times Beer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count total images"
      ],
      "metadata": {
        "id": "HjMoHXnMh0qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for brand in os.listdir(f'scraping/data/folders'):\n",
        "  count+= len(os.listdir(f'scraping/data/folders/{brand}'))\n",
        "print(count/len(os.listdir(f'scraping/data/folders')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htzBDgggh3a6",
        "outputId": "dd32ff28-ad28-4245-e3a1-33761096f7d5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acoMh_htlAIQ"
      },
      "source": [
        "## Classification des bières\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_i1o3jCflAIR"
      },
      "outputs": [],
      "source": [
        "#!conda activate cs_td2\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sys\n",
        "from torchvision import datasets, transforms, models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paramètres\n",
        "BATCH_SIZE = 32\n",
        "NB_EPOCHS = 30\n",
        "LEARNING_RATE = 0.001\n",
        "TEST_SIZE = 0.2\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "size = [224, 224]"
      ],
      "metadata": {
        "id": "JxCQ8rr17Kpp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformations à appliquer (différentes pour le train, le val et le test)"
      ],
      "metadata": {
        "id": "fg33Qx8M1lwh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "CJ0gqlFVlAIR"
      },
      "outputs": [],
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.RandomApply(transforms=[transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))], p=0.1),\n",
        "        transforms.RandomApply(transforms=[transforms.RandomRotation(degrees=(0, 180))], p=0.1),\n",
        "        transforms.RandomApply(transforms=[transforms.RandomInvert()], p=0.1),\n",
        "        transforms.RandomApply(transforms=[transforms.RandomPosterize(bits=2)], p=0.1),\n",
        "        transforms.RandomApply(transforms=[transforms.RandomCrop(size=(64, 64)), transforms.Resize(size)], p=0.1),\n",
        "        transforms.RandomApply(transforms=[transforms.RandomAdjustSharpness(sharpness_factor=2)], p=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)\n",
        "        ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "QJKwlPbtlAIS"
      },
      "outputs": [],
      "source": [
        "# on lit une première fois les images du dataset\n",
        "image_directory = \"./scraping/data/folders\"\n",
        "dataset_full = datasets.ImageFolder(image_directory, data_transforms['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhMvl4QLlAIS",
        "outputId": "71ef3a14-e89d-41fc-c2ce-536425e865f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre d'images de train : 5285\n",
            "Nombre d'images de val : 1322\n",
            "Nombre d'images de test : 2203\n"
          ]
        }
      ],
      "source": [
        "# on split en train, val et test à partir de la liste complète\n",
        "np.random.seed(42)\n",
        "samples_train, samples_test = train_test_split(dataset_full.samples)\n",
        "samples_train, samples_val = train_test_split(samples_train,test_size=TEST_SIZE)\n",
        "\n",
        "print(\"Nombre d'images de train : %i\" % len(samples_train))\n",
        "print(\"Nombre d'images de val : %i\" % len(samples_val))\n",
        "print(\"Nombre d'images de test : %i\" % len(samples_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMteTUTWlAIS",
        "outputId": "3d963e25-9534-44e1-9a0a-7c67b347ebae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f7e07c68950>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# on définit les datasets et loaders pytorch à partir des listes d'images de train / val / test\n",
        "dataset_train = datasets.ImageFolder(image_directory, data_transforms['train'])\n",
        "dataset_train.samples = samples_train\n",
        "dataset_train.imgs = samples_train\n",
        "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "dataset_val = datasets.ImageFolder(image_directory, data_transforms['val'])\n",
        "dataset_val.samples = samples_val\n",
        "dataset_val.imgs = samples_val\n",
        "\n",
        "dataset_test = datasets.ImageFolder(image_directory, data_transforms['test'])\n",
        "dataset_test.samples = samples_test\n",
        "dataset_test.imgs = samples_test\n",
        "\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSPEGNvclAIT",
        "outputId": "513711ba-322d-4b41-f41c-efe6a882208a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apprentissage sur 100 classes\n"
          ]
        }
      ],
      "source": [
        "#On définit les labels\n",
        "labels=range(len(os.listdir(f'scraping/data/folders'))) #nombre de classes\n",
        "if np.min(labels) != 0:\n",
        "    print(\"Error: labels should start at 0 (min is %i)\" % np.min(labels))\n",
        "    sys.exit(-1)\n",
        "if np.max(labels) != (len(np.unique(labels))-1):\n",
        "    print(\"Error: labels should go from 0 to Nclasses (max label = {}; Nclasse = {})\".format(np.max(labels),len(np.unique(labels)))  )\n",
        "    sys.exit(-1)\n",
        "nb_classes = np.max(labels)+1\n",
        "print(f\"Apprentissage sur {nb_classes} classes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE7YI7uBl6SS",
        "outputId": "9b6de3f1-9a99-48f1-e290-522818d97326"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "5Bvz_25OlAIU"
      },
      "outputs": [],
      "source": [
        " # Fonction d'évaluation\n",
        "def evaluate(model, dataset):\n",
        "    avg_loss = 0.\n",
        "    avg_accuracy = 0\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "    for data in loader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        n_correct = torch.sum(preds == labels)\n",
        "        \n",
        "        avg_loss += loss.item()\n",
        "        avg_accuracy += n_correct\n",
        "        \n",
        "    return avg_loss / len(dataset), float(avg_accuracy) / len(dataset)\n",
        "\n",
        "# fonction d'entraînement \n",
        "PRINT_LOSS = True\n",
        "\n",
        "def train_model(model, loader_train, data_val, optimizer, criterion, n_epochs=NB_EPOCHS):\n",
        "    accuracies = []\n",
        "    losses = [] \n",
        "    for epoch in range(n_epochs): \n",
        "        print(f\"EPOCH {epoch} / {NB_EPOCHS - 1}\")\n",
        "        running_loss=0  \n",
        "        correct=0\n",
        "        total=0\n",
        "\n",
        "        for i, data in enumerate(loader_train): # itère sur les minibatchs via le loader apprentissage\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device) # on passe les données sur CPU / GPU\n",
        "            optimizer.zero_grad() # on réinitialise les gradients\n",
        "            outputs = model(inputs) # on calcule l'output\n",
        "            \n",
        "            loss = criterion(outputs, labels) # on calcule la loss\n",
        "            if PRINT_LOSS:\n",
        "                model.train(False)\n",
        "                loss_val, accuracy = evaluate(model, data_val)\n",
        "                model.train(True)\n",
        "                print(\"{} loss train: {:1.4f}\\t val {:1.4f}\\tAcc (val): {:.1%}\".format(i, loss.item(), loss_val, accuracy   ))\n",
        "            \n",
        "            loss.backward() # on effectue la backprop pour calculer les gradients\n",
        "            optimizer.step() # on update les gradients en fonction des paramètres\n",
        "        accuracies.append(accuracy)\n",
        "        losses.append(loss_val)\n",
        "\n",
        "    return losses, accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transfert learning\n"
      ],
      "metadata": {
        "id": "wTAIRnkv2pTQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnYlVA7JlAIU",
        "outputId": "8ce3e2c4-7535-499f-f2c4-c482ceed791a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Récupération du ResNet-18 pré-entraîné...\n",
            "Apprentissage en transfer learning\n",
            "EPOCH 0 / 29\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss train: 4.8467\t val 0.3003\tAcc (val): 0.8%\n",
            "1 loss train: 4.7589\t val 0.2999\tAcc (val): 0.8%\n",
            "2 loss train: 4.6904\t val 0.2994\tAcc (val): 0.8%\n",
            "3 loss train: 4.7136\t val 0.2988\tAcc (val): 1.0%\n",
            "4 loss train: 4.7064\t val 0.2984\tAcc (val): 0.8%\n",
            "5 loss train: 4.7665\t val 0.2980\tAcc (val): 1.0%\n",
            "6 loss train: 4.8996\t val 0.2977\tAcc (val): 1.0%\n",
            "7 loss train: 4.9259\t val 0.2974\tAcc (val): 1.1%\n",
            "8 loss train: 4.7576\t val 0.2971\tAcc (val): 1.1%\n",
            "9 loss train: 4.8114\t val 0.2967\tAcc (val): 1.2%\n",
            "10 loss train: 4.8650\t val 0.2962\tAcc (val): 1.1%\n",
            "11 loss train: 4.7415\t val 0.2957\tAcc (val): 1.1%\n",
            "12 loss train: 4.6817\t val 0.2952\tAcc (val): 1.2%\n",
            "13 loss train: 4.7600\t val 0.2948\tAcc (val): 1.1%\n",
            "14 loss train: 4.5715\t val 0.2943\tAcc (val): 1.3%\n",
            "15 loss train: 4.6791\t val 0.2939\tAcc (val): 1.5%\n",
            "16 loss train: 4.6937\t val 0.2935\tAcc (val): 1.9%\n",
            "17 loss train: 4.6552\t val 0.2930\tAcc (val): 2.2%\n",
            "18 loss train: 4.6876\t val 0.2926\tAcc (val): 2.0%\n",
            "19 loss train: 4.6795\t val 0.2923\tAcc (val): 2.0%\n",
            "20 loss train: 4.8020\t val 0.2919\tAcc (val): 2.0%\n",
            "21 loss train: 4.7381\t val 0.2916\tAcc (val): 2.1%\n",
            "22 loss train: 4.4147\t val 0.2912\tAcc (val): 2.2%\n",
            "23 loss train: 4.5545\t val 0.2909\tAcc (val): 2.4%\n",
            "24 loss train: 4.7113\t val 0.2906\tAcc (val): 2.4%\n",
            "25 loss train: 4.5697\t val 0.2903\tAcc (val): 2.4%\n",
            "26 loss train: 4.5299\t val 0.2900\tAcc (val): 2.3%\n",
            "27 loss train: 4.5942\t val 0.2896\tAcc (val): 2.5%\n",
            "28 loss train: 4.7447\t val 0.2894\tAcc (val): 2.6%\n",
            "29 loss train: 4.5376\t val 0.2891\tAcc (val): 2.3%\n",
            "30 loss train: 4.5765\t val 0.2888\tAcc (val): 2.6%\n",
            "31 loss train: 4.4037\t val 0.2884\tAcc (val): 2.6%\n",
            "32 loss train: 4.5835\t val 0.2881\tAcc (val): 2.7%\n",
            "33 loss train: 4.6740\t val 0.2877\tAcc (val): 2.6%\n",
            "34 loss train: 4.7775\t val 0.2874\tAcc (val): 2.6%\n",
            "35 loss train: 4.6996\t val 0.2872\tAcc (val): 3.0%\n",
            "36 loss train: 4.3376\t val 0.2870\tAcc (val): 3.0%\n",
            "37 loss train: 4.5380\t val 0.2868\tAcc (val): 2.9%\n",
            "38 loss train: 4.4186\t val 0.2866\tAcc (val): 2.7%\n",
            "39 loss train: 4.6299\t val 0.2864\tAcc (val): 2.9%\n",
            "40 loss train: 4.6350\t val 0.2862\tAcc (val): 2.7%\n",
            "41 loss train: 4.4357\t val 0.2860\tAcc (val): 2.7%\n",
            "42 loss train: 4.5632\t val 0.2858\tAcc (val): 2.8%\n",
            "43 loss train: 4.4593\t val 0.2856\tAcc (val): 2.6%\n",
            "44 loss train: 4.4295\t val 0.2854\tAcc (val): 2.4%\n",
            "45 loss train: 4.5482\t val 0.2853\tAcc (val): 2.7%\n",
            "46 loss train: 4.4132\t val 0.2850\tAcc (val): 3.0%\n",
            "47 loss train: 4.6604\t val 0.2849\tAcc (val): 2.8%\n",
            "48 loss train: 4.3821\t val 0.2845\tAcc (val): 3.2%\n",
            "49 loss train: 4.5134\t val 0.2843\tAcc (val): 3.2%\n",
            "50 loss train: 4.6036\t val 0.2842\tAcc (val): 3.1%\n",
            "51 loss train: 4.5051\t val 0.2839\tAcc (val): 3.2%\n",
            "52 loss train: 4.5108\t val 0.2838\tAcc (val): 3.3%\n",
            "53 loss train: 4.4047\t val 0.2836\tAcc (val): 3.5%\n",
            "54 loss train: 4.4128\t val 0.2835\tAcc (val): 3.6%\n",
            "55 loss train: 4.6652\t val 0.2833\tAcc (val): 3.6%\n",
            "56 loss train: 4.6729\t val 0.2831\tAcc (val): 3.8%\n",
            "57 loss train: 4.7580\t val 0.2829\tAcc (val): 3.8%\n",
            "58 loss train: 4.4715\t val 0.2827\tAcc (val): 3.8%\n",
            "59 loss train: 4.6768\t val 0.2824\tAcc (val): 3.7%\n",
            "60 loss train: 4.4738\t val 0.2822\tAcc (val): 3.6%\n",
            "61 loss train: 4.5936\t val 0.2820\tAcc (val): 3.6%\n",
            "62 loss train: 4.4369\t val 0.2817\tAcc (val): 3.5%\n",
            "63 loss train: 4.3899\t val 0.2815\tAcc (val): 3.4%\n",
            "64 loss train: 4.6081\t val 0.2812\tAcc (val): 3.7%\n",
            "65 loss train: 4.3848\t val 0.2810\tAcc (val): 3.9%\n",
            "66 loss train: 4.5529\t val 0.2807\tAcc (val): 4.1%\n",
            "67 loss train: 4.6254\t val 0.2804\tAcc (val): 3.9%\n",
            "68 loss train: 4.5496\t val 0.2802\tAcc (val): 3.9%\n",
            "69 loss train: 4.4165\t val 0.2800\tAcc (val): 3.9%\n",
            "70 loss train: 4.2526\t val 0.2796\tAcc (val): 4.2%\n",
            "71 loss train: 4.3786\t val 0.2794\tAcc (val): 4.2%\n",
            "72 loss train: 4.6300\t val 0.2791\tAcc (val): 4.2%\n",
            "73 loss train: 4.4213\t val 0.2788\tAcc (val): 4.3%\n",
            "74 loss train: 4.4358\t val 0.2785\tAcc (val): 4.6%\n",
            "75 loss train: 4.2577\t val 0.2782\tAcc (val): 4.5%\n",
            "76 loss train: 4.3259\t val 0.2778\tAcc (val): 4.7%\n",
            "77 loss train: 4.3847\t val 0.2777\tAcc (val): 4.2%\n",
            "78 loss train: 4.4295\t val 0.2775\tAcc (val): 4.6%\n",
            "79 loss train: 4.5170\t val 0.2772\tAcc (val): 4.5%\n",
            "80 loss train: 4.4397\t val 0.2769\tAcc (val): 4.3%\n",
            "81 loss train: 4.2889\t val 0.2766\tAcc (val): 4.2%\n",
            "82 loss train: 4.4543\t val 0.2763\tAcc (val): 4.2%\n",
            "83 loss train: 4.4472\t val 0.2760\tAcc (val): 4.5%\n",
            "84 loss train: 4.6451\t val 0.2758\tAcc (val): 4.5%\n",
            "85 loss train: 4.3776\t val 0.2756\tAcc (val): 4.6%\n",
            "86 loss train: 4.4833\t val 0.2754\tAcc (val): 4.5%\n",
            "87 loss train: 4.3202\t val 0.2751\tAcc (val): 4.5%\n",
            "88 loss train: 4.3453\t val 0.2749\tAcc (val): 4.7%\n",
            "89 loss train: 4.3575\t val 0.2746\tAcc (val): 4.8%\n",
            "90 loss train: 4.4013\t val 0.2744\tAcc (val): 4.8%\n",
            "91 loss train: 4.4155\t val 0.2742\tAcc (val): 4.9%\n",
            "92 loss train: 4.2645\t val 0.2739\tAcc (val): 5.2%\n",
            "93 loss train: 4.4504\t val 0.2737\tAcc (val): 5.5%\n",
            "94 loss train: 4.2643\t val 0.2733\tAcc (val): 6.3%\n",
            "95 loss train: 4.3625\t val 0.2730\tAcc (val): 6.7%\n",
            "96 loss train: 4.0985\t val 0.2727\tAcc (val): 7.2%\n",
            "97 loss train: 4.3943\t val 0.2724\tAcc (val): 6.9%\n",
            "98 loss train: 4.4378\t val 0.2722\tAcc (val): 6.7%\n",
            "99 loss train: 4.4444\t val 0.2720\tAcc (val): 7.0%\n",
            "100 loss train: 4.3075\t val 0.2718\tAcc (val): 6.6%\n",
            "101 loss train: 4.1138\t val 0.2716\tAcc (val): 6.7%\n",
            "102 loss train: 4.4205\t val 0.2714\tAcc (val): 6.8%\n",
            "103 loss train: 4.2738\t val 0.2713\tAcc (val): 6.4%\n",
            "104 loss train: 4.4880\t val 0.2712\tAcc (val): 6.2%\n",
            "105 loss train: 4.3229\t val 0.2710\tAcc (val): 6.2%\n",
            "106 loss train: 4.5150\t val 0.2708\tAcc (val): 6.2%\n",
            "107 loss train: 4.3963\t val 0.2706\tAcc (val): 6.4%\n",
            "108 loss train: 4.4898\t val 0.2705\tAcc (val): 6.5%\n",
            "109 loss train: 4.4462\t val 0.2703\tAcc (val): 6.6%\n",
            "110 loss train: 4.2166\t val 0.2702\tAcc (val): 6.6%\n",
            "111 loss train: 4.4220\t val 0.2700\tAcc (val): 6.5%\n",
            "112 loss train: 4.3933\t val 0.2698\tAcc (val): 6.7%\n",
            "113 loss train: 4.3361\t val 0.2696\tAcc (val): 6.7%\n",
            "114 loss train: 4.2996\t val 0.2694\tAcc (val): 6.8%\n",
            "115 loss train: 4.4158\t val 0.2692\tAcc (val): 7.3%\n",
            "116 loss train: 4.3125\t val 0.2690\tAcc (val): 7.8%\n",
            "117 loss train: 4.5732\t val 0.2688\tAcc (val): 7.8%\n",
            "118 loss train: 4.1744\t val 0.2686\tAcc (val): 7.9%\n",
            "119 loss train: 4.1091\t val 0.2684\tAcc (val): 7.7%\n",
            "120 loss train: 4.3600\t val 0.2682\tAcc (val): 7.6%\n",
            "121 loss train: 4.4590\t val 0.2680\tAcc (val): 7.5%\n",
            "122 loss train: 4.2349\t val 0.2677\tAcc (val): 7.9%\n",
            "123 loss train: 4.2866\t val 0.2675\tAcc (val): 8.0%\n",
            "124 loss train: 4.5540\t val 0.2674\tAcc (val): 7.6%\n",
            "125 loss train: 4.3098\t val 0.2671\tAcc (val): 7.6%\n",
            "126 loss train: 4.3627\t val 0.2669\tAcc (val): 7.9%\n",
            "127 loss train: 4.2925\t val 0.2665\tAcc (val): 7.8%\n",
            "128 loss train: 4.3831\t val 0.2663\tAcc (val): 8.2%\n",
            "129 loss train: 4.3198\t val 0.2661\tAcc (val): 8.2%\n",
            "130 loss train: 4.1277\t val 0.2659\tAcc (val): 8.2%\n",
            "131 loss train: 4.2518\t val 0.2658\tAcc (val): 8.5%\n",
            "132 loss train: 4.1630\t val 0.2656\tAcc (val): 8.8%\n",
            "133 loss train: 4.1427\t val 0.2653\tAcc (val): 9.0%\n",
            "134 loss train: 4.2636\t val 0.2651\tAcc (val): 9.2%\n",
            "135 loss train: 4.1485\t val 0.2649\tAcc (val): 9.2%\n",
            "136 loss train: 4.2213\t val 0.2646\tAcc (val): 9.1%\n",
            "137 loss train: 4.1202\t val 0.2645\tAcc (val): 9.2%\n",
            "138 loss train: 4.0642\t val 0.2644\tAcc (val): 9.2%\n",
            "139 loss train: 4.2625\t val 0.2641\tAcc (val): 9.4%\n",
            "140 loss train: 4.2124\t val 0.2640\tAcc (val): 9.3%\n",
            "141 loss train: 4.2591\t val 0.2639\tAcc (val): 9.5%\n",
            "142 loss train: 4.2291\t val 0.2638\tAcc (val): 9.2%\n",
            "143 loss train: 4.3097\t val 0.2636\tAcc (val): 9.1%\n",
            "144 loss train: 4.2376\t val 0.2634\tAcc (val): 9.5%\n",
            "145 loss train: 4.4243\t val 0.2633\tAcc (val): 9.6%\n",
            "146 loss train: 3.9744\t val 0.2630\tAcc (val): 9.5%\n",
            "147 loss train: 4.4730\t val 0.2628\tAcc (val): 9.8%\n",
            "148 loss train: 4.0534\t val 0.2625\tAcc (val): 9.8%\n",
            "149 loss train: 4.3132\t val 0.2622\tAcc (val): 10.2%\n",
            "150 loss train: 4.1502\t val 0.2619\tAcc (val): 10.5%\n",
            "151 loss train: 4.3622\t val 0.2618\tAcc (val): 10.6%\n",
            "152 loss train: 4.3935\t val 0.2617\tAcc (val): 10.7%\n",
            "153 loss train: 4.3054\t val 0.2613\tAcc (val): 11.3%\n",
            "154 loss train: 4.2424\t val 0.2611\tAcc (val): 11.0%\n",
            "155 loss train: 4.1287\t val 0.2609\tAcc (val): 11.2%\n",
            "156 loss train: 4.2786\t val 0.2607\tAcc (val): 11.1%\n",
            "157 loss train: 3.9677\t val 0.2605\tAcc (val): 11.0%\n",
            "158 loss train: 4.2282\t val 0.2603\tAcc (val): 10.8%\n",
            "159 loss train: 4.2787\t val 0.2601\tAcc (val): 10.9%\n",
            "160 loss train: 4.3072\t val 0.2600\tAcc (val): 11.1%\n",
            "161 loss train: 4.4205\t val 0.2597\tAcc (val): 11.1%\n",
            "162 loss train: 4.3613\t val 0.2595\tAcc (val): 10.9%\n",
            "163 loss train: 4.3482\t val 0.2593\tAcc (val): 11.2%\n",
            "164 loss train: 4.3149\t val 0.2592\tAcc (val): 11.0%\n",
            "165 loss train: 4.4085\t val 0.2590\tAcc (val): 11.0%\n",
            "EPOCH 1 / 29\n",
            "0 loss train: 4.0878\t val 0.2588\tAcc (val): 10.8%\n",
            "1 loss train: 4.0584\t val 0.2588\tAcc (val): 11.3%\n",
            "2 loss train: 4.0042\t val 0.2587\tAcc (val): 11.4%\n",
            "3 loss train: 4.0186\t val 0.2587\tAcc (val): 11.2%\n",
            "4 loss train: 4.1000\t val 0.2586\tAcc (val): 11.3%\n",
            "5 loss train: 4.3071\t val 0.2585\tAcc (val): 11.2%\n",
            "6 loss train: 3.9040\t val 0.2584\tAcc (val): 11.0%\n",
            "7 loss train: 4.3022\t val 0.2582\tAcc (val): 11.1%\n",
            "8 loss train: 4.1595\t val 0.2583\tAcc (val): 11.1%\n",
            "9 loss train: 4.0994\t val 0.2581\tAcc (val): 11.5%\n",
            "10 loss train: 3.9134\t val 0.2581\tAcc (val): 11.9%\n",
            "11 loss train: 4.1142\t val 0.2582\tAcc (val): 11.5%\n",
            "12 loss train: 4.1387\t val 0.2581\tAcc (val): 11.3%\n",
            "13 loss train: 4.3101\t val 0.2579\tAcc (val): 11.4%\n",
            "14 loss train: 4.2169\t val 0.2575\tAcc (val): 11.4%\n",
            "15 loss train: 4.3257\t val 0.2572\tAcc (val): 11.7%\n",
            "16 loss train: 4.1549\t val 0.2570\tAcc (val): 11.5%\n",
            "17 loss train: 4.4599\t val 0.2566\tAcc (val): 11.6%\n",
            "18 loss train: 3.9192\t val 0.2563\tAcc (val): 11.9%\n",
            "19 loss train: 4.3297\t val 0.2560\tAcc (val): 12.0%\n",
            "20 loss train: 4.0347\t val 0.2557\tAcc (val): 12.0%\n",
            "21 loss train: 4.2755\t val 0.2555\tAcc (val): 12.1%\n",
            "22 loss train: 4.0742\t val 0.2553\tAcc (val): 12.8%\n",
            "23 loss train: 4.0280\t val 0.2551\tAcc (val): 13.2%\n",
            "24 loss train: 4.0618\t val 0.2551\tAcc (val): 13.4%\n",
            "25 loss train: 4.1811\t val 0.2549\tAcc (val): 12.8%\n",
            "26 loss train: 4.1569\t val 0.2547\tAcc (val): 12.7%\n",
            "27 loss train: 4.3195\t val 0.2545\tAcc (val): 12.6%\n",
            "28 loss train: 4.1620\t val 0.2544\tAcc (val): 12.6%\n",
            "29 loss train: 3.8703\t val 0.2542\tAcc (val): 12.8%\n",
            "30 loss train: 4.4024\t val 0.2539\tAcc (val): 12.8%\n",
            "31 loss train: 4.1758\t val 0.2537\tAcc (val): 13.1%\n",
            "32 loss train: 3.9392\t val 0.2536\tAcc (val): 13.1%\n",
            "33 loss train: 4.1788\t val 0.2535\tAcc (val): 13.3%\n",
            "34 loss train: 4.2293\t val 0.2532\tAcc (val): 13.2%\n",
            "35 loss train: 4.2082\t val 0.2530\tAcc (val): 13.7%\n",
            "36 loss train: 4.0835\t val 0.2527\tAcc (val): 13.7%\n",
            "37 loss train: 4.2055\t val 0.2526\tAcc (val): 13.8%\n",
            "38 loss train: 4.0603\t val 0.2523\tAcc (val): 14.1%\n",
            "39 loss train: 4.0838\t val 0.2520\tAcc (val): 14.1%\n",
            "40 loss train: 4.3312\t val 0.2518\tAcc (val): 14.2%\n",
            "41 loss train: 4.1175\t val 0.2516\tAcc (val): 14.4%\n",
            "42 loss train: 4.1412\t val 0.2514\tAcc (val): 14.4%\n",
            "43 loss train: 3.8914\t val 0.2513\tAcc (val): 14.5%\n",
            "44 loss train: 4.3384\t val 0.2510\tAcc (val): 14.6%\n",
            "45 loss train: 4.1984\t val 0.2510\tAcc (val): 14.4%\n",
            "46 loss train: 3.9349\t val 0.2507\tAcc (val): 14.6%\n",
            "47 loss train: 4.2465\t val 0.2505\tAcc (val): 14.2%\n",
            "48 loss train: 4.2857\t val 0.2504\tAcc (val): 14.7%\n",
            "49 loss train: 4.1757\t val 0.2501\tAcc (val): 14.7%\n",
            "50 loss train: 4.0464\t val 0.2500\tAcc (val): 15.3%\n",
            "51 loss train: 4.1482\t val 0.2497\tAcc (val): 14.9%\n",
            "52 loss train: 3.9643\t val 0.2495\tAcc (val): 14.7%\n",
            "53 loss train: 4.1458\t val 0.2493\tAcc (val): 14.8%\n",
            "54 loss train: 4.2210\t val 0.2492\tAcc (val): 15.1%\n",
            "55 loss train: 4.1672\t val 0.2490\tAcc (val): 14.5%\n",
            "56 loss train: 3.8912\t val 0.2488\tAcc (val): 14.7%\n",
            "57 loss train: 3.9642\t val 0.2487\tAcc (val): 14.4%\n",
            "58 loss train: 4.0090\t val 0.2485\tAcc (val): 14.6%\n",
            "59 loss train: 4.0086\t val 0.2483\tAcc (val): 14.5%\n",
            "60 loss train: 4.0972\t val 0.2482\tAcc (val): 14.1%\n",
            "61 loss train: 4.1598\t val 0.2481\tAcc (val): 14.2%\n",
            "62 loss train: 4.0746\t val 0.2479\tAcc (val): 14.5%\n",
            "63 loss train: 4.1437\t val 0.2478\tAcc (val): 14.8%\n",
            "64 loss train: 4.0387\t val 0.2477\tAcc (val): 14.4%\n",
            "65 loss train: 4.0799\t val 0.2473\tAcc (val): 14.8%\n",
            "66 loss train: 3.9921\t val 0.2471\tAcc (val): 14.5%\n",
            "67 loss train: 4.0737\t val 0.2470\tAcc (val): 14.8%\n",
            "68 loss train: 3.9439\t val 0.2469\tAcc (val): 14.8%\n",
            "69 loss train: 3.9278\t val 0.2467\tAcc (val): 14.8%\n",
            "70 loss train: 3.9559\t val 0.2466\tAcc (val): 14.6%\n",
            "71 loss train: 4.1990\t val 0.2465\tAcc (val): 14.9%\n",
            "72 loss train: 4.0576\t val 0.2464\tAcc (val): 15.0%\n",
            "73 loss train: 4.0600\t val 0.2460\tAcc (val): 15.2%\n",
            "74 loss train: 4.0059\t val 0.2459\tAcc (val): 15.2%\n",
            "75 loss train: 4.1414\t val 0.2456\tAcc (val): 15.4%\n",
            "76 loss train: 4.0544\t val 0.2453\tAcc (val): 16.0%\n",
            "77 loss train: 4.0210\t val 0.2450\tAcc (val): 16.6%\n",
            "78 loss train: 3.6517\t val 0.2448\tAcc (val): 17.2%\n",
            "79 loss train: 3.9684\t val 0.2445\tAcc (val): 17.5%\n",
            "80 loss train: 4.1504\t val 0.2443\tAcc (val): 17.5%\n",
            "81 loss train: 4.0588\t val 0.2440\tAcc (val): 17.7%\n",
            "82 loss train: 3.8792\t val 0.2440\tAcc (val): 17.5%\n",
            "83 loss train: 3.9481\t val 0.2439\tAcc (val): 17.7%\n",
            "84 loss train: 3.9934\t val 0.2437\tAcc (val): 17.9%\n",
            "85 loss train: 3.9987\t val 0.2435\tAcc (val): 18.0%\n",
            "86 loss train: 4.0491\t val 0.2433\tAcc (val): 17.9%\n",
            "87 loss train: 3.7964\t val 0.2431\tAcc (val): 17.9%\n",
            "88 loss train: 3.9605\t val 0.2431\tAcc (val): 18.1%\n",
            "89 loss train: 4.0319\t val 0.2427\tAcc (val): 18.2%\n",
            "90 loss train: 3.9870\t val 0.2426\tAcc (val): 17.9%\n",
            "91 loss train: 4.0015\t val 0.2427\tAcc (val): 17.5%\n",
            "92 loss train: 4.0982\t val 0.2425\tAcc (val): 17.1%\n",
            "93 loss train: 4.0537\t val 0.2425\tAcc (val): 17.2%\n",
            "94 loss train: 3.8768\t val 0.2423\tAcc (val): 17.2%\n",
            "95 loss train: 4.2651\t val 0.2421\tAcc (val): 16.6%\n",
            "96 loss train: 3.9119\t val 0.2419\tAcc (val): 16.8%\n",
            "97 loss train: 4.0959\t val 0.2416\tAcc (val): 16.8%\n",
            "98 loss train: 4.1136\t val 0.2415\tAcc (val): 16.5%\n",
            "99 loss train: 3.8329\t val 0.2412\tAcc (val): 16.7%\n",
            "100 loss train: 3.8962\t val 0.2409\tAcc (val): 16.9%\n",
            "101 loss train: 3.8208\t val 0.2407\tAcc (val): 17.1%\n",
            "102 loss train: 3.9468\t val 0.2403\tAcc (val): 17.2%\n",
            "103 loss train: 3.8836\t val 0.2400\tAcc (val): 17.1%\n",
            "104 loss train: 4.0210\t val 0.2399\tAcc (val): 17.6%\n",
            "105 loss train: 3.9184\t val 0.2398\tAcc (val): 17.9%\n",
            "106 loss train: 3.7147\t val 0.2395\tAcc (val): 18.3%\n",
            "107 loss train: 4.0027\t val 0.2393\tAcc (val): 18.7%\n",
            "108 loss train: 3.9373\t val 0.2391\tAcc (val): 18.5%\n",
            "109 loss train: 4.0482\t val 0.2389\tAcc (val): 18.7%\n",
            "110 loss train: 3.7436\t val 0.2387\tAcc (val): 18.5%\n",
            "111 loss train: 3.9154\t val 0.2385\tAcc (val): 18.1%\n",
            "112 loss train: 4.0020\t val 0.2384\tAcc (val): 18.2%\n",
            "113 loss train: 3.7691\t val 0.2384\tAcc (val): 18.1%\n",
            "114 loss train: 3.8726\t val 0.2383\tAcc (val): 17.9%\n",
            "115 loss train: 3.9621\t val 0.2382\tAcc (val): 18.1%\n",
            "116 loss train: 3.9764\t val 0.2382\tAcc (val): 18.0%\n",
            "117 loss train: 3.8575\t val 0.2380\tAcc (val): 18.0%\n",
            "118 loss train: 3.8494\t val 0.2378\tAcc (val): 18.2%\n",
            "119 loss train: 3.7776\t val 0.2375\tAcc (val): 18.3%\n",
            "120 loss train: 3.9297\t val 0.2374\tAcc (val): 18.2%\n",
            "121 loss train: 4.0475\t val 0.2373\tAcc (val): 18.2%\n",
            "122 loss train: 3.9625\t val 0.2373\tAcc (val): 19.1%\n",
            "123 loss train: 3.9612\t val 0.2372\tAcc (val): 19.7%\n",
            "124 loss train: 4.0309\t val 0.2371\tAcc (val): 20.0%\n",
            "125 loss train: 3.7294\t val 0.2370\tAcc (val): 20.3%\n",
            "126 loss train: 3.9525\t val 0.2367\tAcc (val): 20.4%\n",
            "127 loss train: 3.7982\t val 0.2367\tAcc (val): 20.7%\n",
            "128 loss train: 3.9452\t val 0.2365\tAcc (val): 20.3%\n",
            "129 loss train: 4.1982\t val 0.2362\tAcc (val): 19.9%\n",
            "130 loss train: 3.8192\t val 0.2361\tAcc (val): 19.6%\n",
            "131 loss train: 3.6529\t val 0.2360\tAcc (val): 19.7%\n",
            "132 loss train: 3.6948\t val 0.2359\tAcc (val): 19.7%\n",
            "133 loss train: 3.9549\t val 0.2357\tAcc (val): 19.7%\n",
            "134 loss train: 3.7761\t val 0.2354\tAcc (val): 19.9%\n",
            "135 loss train: 4.0056\t val 0.2352\tAcc (val): 19.9%\n",
            "136 loss train: 3.6491\t val 0.2349\tAcc (val): 19.8%\n",
            "137 loss train: 4.1476\t val 0.2348\tAcc (val): 19.6%\n",
            "138 loss train: 4.1318\t val 0.2346\tAcc (val): 19.6%\n",
            "139 loss train: 4.1801\t val 0.2344\tAcc (val): 19.4%\n",
            "140 loss train: 3.6964\t val 0.2343\tAcc (val): 19.7%\n",
            "141 loss train: 3.3691\t val 0.2343\tAcc (val): 20.2%\n",
            "142 loss train: 4.0291\t val 0.2343\tAcc (val): 20.3%\n",
            "143 loss train: 3.7741\t val 0.2340\tAcc (val): 20.6%\n",
            "144 loss train: 3.9106\t val 0.2338\tAcc (val): 20.7%\n",
            "145 loss train: 3.7680\t val 0.2337\tAcc (val): 21.1%\n",
            "146 loss train: 3.8557\t val 0.2335\tAcc (val): 21.0%\n",
            "147 loss train: 3.7863\t val 0.2333\tAcc (val): 21.0%\n",
            "148 loss train: 3.6679\t val 0.2332\tAcc (val): 21.0%\n",
            "149 loss train: 3.8855\t val 0.2331\tAcc (val): 20.4%\n",
            "150 loss train: 3.8699\t val 0.2328\tAcc (val): 20.3%\n",
            "151 loss train: 3.9700\t val 0.2327\tAcc (val): 20.5%\n",
            "152 loss train: 3.8473\t val 0.2327\tAcc (val): 20.7%\n",
            "153 loss train: 3.9998\t val 0.2323\tAcc (val): 21.0%\n",
            "154 loss train: 3.9231\t val 0.2319\tAcc (val): 21.2%\n",
            "155 loss train: 3.4510\t val 0.2317\tAcc (val): 21.5%\n",
            "156 loss train: 3.7874\t val 0.2315\tAcc (val): 21.6%\n",
            "157 loss train: 3.8369\t val 0.2315\tAcc (val): 21.6%\n",
            "158 loss train: 3.7040\t val 0.2315\tAcc (val): 21.3%\n",
            "159 loss train: 3.7775\t val 0.2314\tAcc (val): 21.5%\n",
            "160 loss train: 3.6643\t val 0.2312\tAcc (val): 21.3%\n",
            "161 loss train: 3.6031\t val 0.2311\tAcc (val): 21.2%\n",
            "162 loss train: 3.6166\t val 0.2308\tAcc (val): 21.1%\n",
            "163 loss train: 3.9280\t val 0.2307\tAcc (val): 21.3%\n",
            "164 loss train: 3.8418\t val 0.2306\tAcc (val): 21.3%\n",
            "165 loss train: 4.4085\t val 0.2307\tAcc (val): 21.3%\n",
            "EPOCH 2 / 29\n",
            "0 loss train: 3.8273\t val 0.2305\tAcc (val): 21.2%\n",
            "1 loss train: 3.6460\t val 0.2303\tAcc (val): 21.0%\n",
            "2 loss train: 3.6721\t val 0.2303\tAcc (val): 20.9%\n",
            "3 loss train: 3.8125\t val 0.2304\tAcc (val): 20.8%\n",
            "4 loss train: 3.7258\t val 0.2305\tAcc (val): 21.3%\n",
            "5 loss train: 3.7758\t val 0.2303\tAcc (val): 21.4%\n",
            "6 loss train: 3.6192\t val 0.2302\tAcc (val): 21.3%\n",
            "7 loss train: 3.5269\t val 0.2301\tAcc (val): 21.4%\n",
            "8 loss train: 3.9485\t val 0.2300\tAcc (val): 21.5%\n",
            "9 loss train: 3.7429\t val 0.2298\tAcc (val): 21.6%\n",
            "10 loss train: 3.9550\t val 0.2298\tAcc (val): 21.7%\n",
            "11 loss train: 3.7106\t val 0.2297\tAcc (val): 21.6%\n",
            "12 loss train: 3.9004\t val 0.2296\tAcc (val): 22.0%\n",
            "13 loss train: 3.6887\t val 0.2294\tAcc (val): 22.5%\n",
            "14 loss train: 3.8343\t val 0.2291\tAcc (val): 22.5%\n",
            "15 loss train: 3.7038\t val 0.2288\tAcc (val): 22.9%\n",
            "16 loss train: 3.7534\t val 0.2288\tAcc (val): 23.2%\n",
            "17 loss train: 3.7712\t val 0.2285\tAcc (val): 23.4%\n",
            "18 loss train: 3.8301\t val 0.2282\tAcc (val): 23.6%\n",
            "19 loss train: 3.5711\t val 0.2280\tAcc (val): 23.4%\n",
            "20 loss train: 3.7070\t val 0.2279\tAcc (val): 23.0%\n",
            "21 loss train: 3.8001\t val 0.2278\tAcc (val): 22.7%\n",
            "22 loss train: 3.6525\t val 0.2278\tAcc (val): 23.2%\n",
            "23 loss train: 3.9131\t val 0.2276\tAcc (val): 23.5%\n",
            "24 loss train: 3.7267\t val 0.2274\tAcc (val): 23.4%\n",
            "25 loss train: 3.5025\t val 0.2274\tAcc (val): 23.1%\n",
            "26 loss train: 3.7280\t val 0.2273\tAcc (val): 23.4%\n",
            "27 loss train: 3.8505\t val 0.2272\tAcc (val): 23.0%\n",
            "28 loss train: 3.6811\t val 0.2270\tAcc (val): 23.1%\n",
            "29 loss train: 3.9035\t val 0.2269\tAcc (val): 23.4%\n",
            "30 loss train: 3.7356\t val 0.2269\tAcc (val): 23.2%\n",
            "31 loss train: 3.6530\t val 0.2268\tAcc (val): 23.1%\n",
            "32 loss train: 3.7556\t val 0.2267\tAcc (val): 23.3%\n",
            "33 loss train: 3.4843\t val 0.2265\tAcc (val): 23.4%\n",
            "34 loss train: 3.7312\t val 0.2264\tAcc (val): 23.3%\n",
            "35 loss train: 3.7437\t val 0.2262\tAcc (val): 23.6%\n",
            "36 loss train: 3.5832\t val 0.2261\tAcc (val): 23.5%\n",
            "37 loss train: 3.9935\t val 0.2259\tAcc (val): 23.3%\n",
            "38 loss train: 3.4917\t val 0.2257\tAcc (val): 23.8%\n",
            "39 loss train: 3.7336\t val 0.2257\tAcc (val): 23.9%\n",
            "40 loss train: 3.8531\t val 0.2256\tAcc (val): 23.6%\n",
            "41 loss train: 3.8118\t val 0.2257\tAcc (val): 23.9%\n",
            "42 loss train: 3.4366\t val 0.2258\tAcc (val): 24.1%\n",
            "43 loss train: 3.7080\t val 0.2257\tAcc (val): 24.1%\n",
            "44 loss train: 3.5396\t val 0.2255\tAcc (val): 24.5%\n",
            "45 loss train: 3.7959\t val 0.2253\tAcc (val): 24.3%\n",
            "46 loss train: 3.7318\t val 0.2251\tAcc (val): 24.4%\n",
            "47 loss train: 3.6097\t val 0.2250\tAcc (val): 24.4%\n",
            "48 loss train: 3.5441\t val 0.2245\tAcc (val): 24.7%\n",
            "49 loss train: 3.9667\t val 0.2244\tAcc (val): 24.8%\n",
            "50 loss train: 3.6722\t val 0.2240\tAcc (val): 24.7%\n",
            "51 loss train: 3.5770\t val 0.2239\tAcc (val): 25.0%\n",
            "52 loss train: 3.6540\t val 0.2237\tAcc (val): 23.9%\n",
            "53 loss train: 3.7094\t val 0.2235\tAcc (val): 24.1%\n",
            "54 loss train: 3.6949\t val 0.2236\tAcc (val): 23.7%\n",
            "55 loss train: 3.9268\t val 0.2233\tAcc (val): 24.0%\n",
            "56 loss train: 3.8290\t val 0.2230\tAcc (val): 23.9%\n",
            "57 loss train: 3.6720\t val 0.2229\tAcc (val): 24.1%\n",
            "58 loss train: 3.8203\t val 0.2226\tAcc (val): 24.4%\n",
            "59 loss train: 3.8304\t val 0.2227\tAcc (val): 24.1%\n",
            "60 loss train: 3.3408\t val 0.2227\tAcc (val): 24.0%\n",
            "61 loss train: 3.5742\t val 0.2226\tAcc (val): 24.1%\n",
            "62 loss train: 3.7851\t val 0.2223\tAcc (val): 24.3%\n",
            "63 loss train: 3.5784\t val 0.2221\tAcc (val): 24.4%\n",
            "64 loss train: 3.8011\t val 0.2217\tAcc (val): 24.6%\n",
            "65 loss train: 3.6054\t val 0.2216\tAcc (val): 24.9%\n",
            "66 loss train: 3.6463\t val 0.2215\tAcc (val): 25.0%\n",
            "67 loss train: 3.6584\t val 0.2213\tAcc (val): 25.2%\n",
            "68 loss train: 3.6798\t val 0.2213\tAcc (val): 25.2%\n",
            "69 loss train: 3.9169\t val 0.2211\tAcc (val): 25.2%\n",
            "70 loss train: 3.8810\t val 0.2209\tAcc (val): 25.6%\n",
            "71 loss train: 3.6457\t val 0.2208\tAcc (val): 26.3%\n",
            "72 loss train: 3.5854\t val 0.2207\tAcc (val): 26.2%\n",
            "73 loss train: 3.6007\t val 0.2205\tAcc (val): 26.4%\n",
            "74 loss train: 3.5884\t val 0.2202\tAcc (val): 26.8%\n",
            "75 loss train: 3.9304\t val 0.2201\tAcc (val): 27.1%\n",
            "76 loss train: 3.5956\t val 0.2200\tAcc (val): 27.1%\n",
            "77 loss train: 3.7132\t val 0.2200\tAcc (val): 26.9%\n",
            "78 loss train: 3.8096\t val 0.2198\tAcc (val): 26.6%\n",
            "79 loss train: 3.8554\t val 0.2197\tAcc (val): 27.3%\n",
            "80 loss train: 3.7510\t val 0.2196\tAcc (val): 26.8%\n",
            "81 loss train: 3.4652\t val 0.2193\tAcc (val): 26.6%\n",
            "82 loss train: 3.5819\t val 0.2191\tAcc (val): 26.2%\n",
            "83 loss train: 4.0834\t val 0.2193\tAcc (val): 26.3%\n",
            "84 loss train: 3.4739\t val 0.2192\tAcc (val): 26.2%\n",
            "85 loss train: 3.4208\t val 0.2191\tAcc (val): 26.2%\n",
            "86 loss train: 3.5272\t val 0.2191\tAcc (val): 25.9%\n",
            "87 loss train: 3.7749\t val 0.2189\tAcc (val): 26.6%\n",
            "88 loss train: 3.6892\t val 0.2188\tAcc (val): 26.6%\n",
            "89 loss train: 3.9112\t val 0.2186\tAcc (val): 26.8%\n",
            "90 loss train: 3.5575\t val 0.2186\tAcc (val): 27.4%\n",
            "91 loss train: 3.5919\t val 0.2187\tAcc (val): 26.6%\n",
            "92 loss train: 3.8226\t val 0.2188\tAcc (val): 26.6%\n",
            "93 loss train: 3.6046\t val 0.2186\tAcc (val): 26.9%\n",
            "94 loss train: 3.6195\t val 0.2185\tAcc (val): 26.9%\n",
            "95 loss train: 3.6339\t val 0.2181\tAcc (val): 27.0%\n",
            "96 loss train: 3.8336\t val 0.2176\tAcc (val): 26.9%\n",
            "97 loss train: 3.6806\t val 0.2175\tAcc (val): 26.8%\n",
            "98 loss train: 3.8994\t val 0.2173\tAcc (val): 26.9%\n",
            "99 loss train: 3.6510\t val 0.2172\tAcc (val): 26.9%\n",
            "100 loss train: 3.5850\t val 0.2171\tAcc (val): 26.9%\n",
            "101 loss train: 3.6832\t val 0.2169\tAcc (val): 27.3%\n",
            "102 loss train: 3.4786\t val 0.2170\tAcc (val): 27.4%\n",
            "103 loss train: 3.6583\t val 0.2165\tAcc (val): 27.5%\n",
            "104 loss train: 3.6477\t val 0.2161\tAcc (val): 27.5%\n",
            "105 loss train: 3.5619\t val 0.2159\tAcc (val): 27.2%\n",
            "106 loss train: 3.5805\t val 0.2160\tAcc (val): 27.1%\n",
            "107 loss train: 3.5139\t val 0.2160\tAcc (val): 26.7%\n",
            "108 loss train: 3.3105\t val 0.2158\tAcc (val): 27.1%\n",
            "109 loss train: 3.6506\t val 0.2158\tAcc (val): 26.8%\n",
            "110 loss train: 3.8466\t val 0.2156\tAcc (val): 26.9%\n",
            "111 loss train: 3.6521\t val 0.2157\tAcc (val): 27.2%\n",
            "112 loss train: 3.7026\t val 0.2156\tAcc (val): 26.9%\n",
            "113 loss train: 3.5216\t val 0.2153\tAcc (val): 27.4%\n",
            "114 loss train: 3.7301\t val 0.2152\tAcc (val): 27.8%\n",
            "115 loss train: 3.4555\t val 0.2149\tAcc (val): 28.3%\n",
            "116 loss train: 3.6941\t val 0.2147\tAcc (val): 28.0%\n",
            "117 loss train: 3.4788\t val 0.2145\tAcc (val): 27.8%\n",
            "118 loss train: 3.7554\t val 0.2143\tAcc (val): 27.8%\n",
            "119 loss train: 3.1924\t val 0.2140\tAcc (val): 28.3%\n",
            "120 loss train: 3.5654\t val 0.2142\tAcc (val): 28.1%\n",
            "121 loss train: 3.5938\t val 0.2143\tAcc (val): 28.1%\n",
            "122 loss train: 3.5912\t val 0.2144\tAcc (val): 27.6%\n",
            "123 loss train: 3.6910\t val 0.2144\tAcc (val): 27.8%\n",
            "124 loss train: 3.4125\t val 0.2144\tAcc (val): 27.9%\n",
            "125 loss train: 3.8400\t val 0.2141\tAcc (val): 27.7%\n",
            "126 loss train: 3.5140\t val 0.2141\tAcc (val): 27.4%\n",
            "127 loss train: 3.9608\t val 0.2139\tAcc (val): 27.2%\n",
            "128 loss train: 3.6004\t val 0.2136\tAcc (val): 26.9%\n",
            "129 loss train: 3.5731\t val 0.2137\tAcc (val): 26.9%\n",
            "130 loss train: 3.6177\t val 0.2135\tAcc (val): 27.5%\n",
            "131 loss train: 3.9250\t val 0.2135\tAcc (val): 27.2%\n",
            "132 loss train: 3.5562\t val 0.2134\tAcc (val): 27.5%\n",
            "133 loss train: 3.6044\t val 0.2131\tAcc (val): 27.2%\n",
            "134 loss train: 3.7978\t val 0.2131\tAcc (val): 26.9%\n",
            "135 loss train: 3.4047\t val 0.2129\tAcc (val): 27.0%\n",
            "136 loss train: 3.4297\t val 0.2127\tAcc (val): 27.5%\n",
            "137 loss train: 3.6281\t val 0.2127\tAcc (val): 27.5%\n",
            "138 loss train: 3.3031\t val 0.2128\tAcc (val): 27.8%\n",
            "139 loss train: 3.1840\t val 0.2129\tAcc (val): 28.4%\n",
            "140 loss train: 3.3039\t val 0.2129\tAcc (val): 28.3%\n",
            "141 loss train: 3.8183\t val 0.2129\tAcc (val): 28.3%\n",
            "142 loss train: 3.5026\t val 0.2127\tAcc (val): 28.4%\n",
            "143 loss train: 3.7946\t val 0.2126\tAcc (val): 28.7%\n",
            "144 loss train: 3.4639\t val 0.2126\tAcc (val): 29.0%\n",
            "145 loss train: 3.3938\t val 0.2123\tAcc (val): 29.3%\n",
            "146 loss train: 3.8263\t val 0.2123\tAcc (val): 29.4%\n",
            "147 loss train: 3.5093\t val 0.2120\tAcc (val): 29.3%\n",
            "148 loss train: 3.7500\t val 0.2117\tAcc (val): 29.2%\n",
            "149 loss train: 3.4858\t val 0.2116\tAcc (val): 29.4%\n",
            "150 loss train: 3.5440\t val 0.2114\tAcc (val): 29.5%\n",
            "151 loss train: 3.6480\t val 0.2113\tAcc (val): 29.3%\n",
            "152 loss train: 3.5686\t val 0.2112\tAcc (val): 29.4%\n",
            "153 loss train: 3.3060\t val 0.2108\tAcc (val): 29.5%\n",
            "154 loss train: 3.4637\t val 0.2105\tAcc (val): 29.1%\n",
            "155 loss train: 3.5649\t val 0.2105\tAcc (val): 28.9%\n",
            "156 loss train: 3.8167\t val 0.2103\tAcc (val): 28.8%\n",
            "157 loss train: 3.5529\t val 0.2102\tAcc (val): 29.4%\n",
            "158 loss train: 3.7034\t val 0.2099\tAcc (val): 29.3%\n",
            "159 loss train: 3.7054\t val 0.2098\tAcc (val): 29.6%\n",
            "160 loss train: 3.5128\t val 0.2092\tAcc (val): 29.2%\n",
            "161 loss train: 3.8411\t val 0.2092\tAcc (val): 29.5%\n",
            "162 loss train: 3.8407\t val 0.2090\tAcc (val): 29.3%\n",
            "163 loss train: 3.4825\t val 0.2089\tAcc (val): 29.2%\n",
            "164 loss train: 3.1738\t val 0.2087\tAcc (val): 29.0%\n",
            "165 loss train: 3.6506\t val 0.2081\tAcc (val): 28.2%\n",
            "EPOCH 3 / 29\n",
            "0 loss train: 3.3753\t val 0.2080\tAcc (val): 28.7%\n",
            "1 loss train: 3.4097\t val 0.2076\tAcc (val): 28.4%\n",
            "2 loss train: 3.6702\t val 0.2076\tAcc (val): 28.6%\n",
            "3 loss train: 3.1746\t val 0.2075\tAcc (val): 28.4%\n",
            "4 loss train: 3.4983\t val 0.2077\tAcc (val): 28.7%\n",
            "5 loss train: 3.2815\t val 0.2077\tAcc (val): 28.9%\n",
            "6 loss train: 3.6598\t val 0.2075\tAcc (val): 28.8%\n",
            "7 loss train: 3.7444\t val 0.2074\tAcc (val): 28.4%\n",
            "8 loss train: 3.1447\t val 0.2072\tAcc (val): 29.0%\n",
            "9 loss train: 3.6106\t val 0.2069\tAcc (val): 28.7%\n",
            "10 loss train: 3.1272\t val 0.2069\tAcc (val): 28.9%\n",
            "11 loss train: 3.4683\t val 0.2069\tAcc (val): 28.7%\n",
            "12 loss train: 3.5022\t val 0.2067\tAcc (val): 28.9%\n",
            "13 loss train: 3.6413\t val 0.2066\tAcc (val): 28.8%\n",
            "14 loss train: 3.4103\t val 0.2066\tAcc (val): 28.8%\n",
            "15 loss train: 3.2281\t val 0.2064\tAcc (val): 28.9%\n",
            "16 loss train: 3.6790\t val 0.2066\tAcc (val): 28.7%\n",
            "17 loss train: 3.3117\t val 0.2067\tAcc (val): 28.5%\n",
            "18 loss train: 3.6057\t val 0.2065\tAcc (val): 29.0%\n",
            "19 loss train: 3.5475\t val 0.2064\tAcc (val): 29.2%\n",
            "20 loss train: 3.5146\t val 0.2063\tAcc (val): 29.2%\n",
            "21 loss train: 3.3246\t val 0.2062\tAcc (val): 29.3%\n",
            "22 loss train: 3.2066\t val 0.2059\tAcc (val): 29.0%\n",
            "23 loss train: 3.5687\t val 0.2058\tAcc (val): 29.2%\n",
            "24 loss train: 3.2766\t val 0.2057\tAcc (val): 29.3%\n",
            "25 loss train: 3.3807\t val 0.2055\tAcc (val): 29.6%\n",
            "26 loss train: 3.5212\t val 0.2052\tAcc (val): 30.0%\n",
            "27 loss train: 3.3627\t val 0.2047\tAcc (val): 29.7%\n",
            "28 loss train: 3.5240\t val 0.2046\tAcc (val): 30.2%\n",
            "29 loss train: 3.3688\t val 0.2046\tAcc (val): 30.2%\n",
            "30 loss train: 3.4726\t val 0.2045\tAcc (val): 30.0%\n",
            "31 loss train: 3.6716\t val 0.2045\tAcc (val): 30.0%\n",
            "32 loss train: 3.5492\t val 0.2043\tAcc (val): 29.9%\n",
            "33 loss train: 3.3356\t val 0.2043\tAcc (val): 30.2%\n",
            "34 loss train: 3.1914\t val 0.2045\tAcc (val): 30.2%\n",
            "35 loss train: 3.4400\t val 0.2044\tAcc (val): 30.6%\n",
            "36 loss train: 3.3701\t val 0.2043\tAcc (val): 30.5%\n",
            "37 loss train: 3.3557\t val 0.2043\tAcc (val): 30.4%\n",
            "38 loss train: 3.2503\t val 0.2043\tAcc (val): 30.1%\n",
            "39 loss train: 3.3702\t val 0.2044\tAcc (val): 30.0%\n",
            "40 loss train: 3.4378\t val 0.2043\tAcc (val): 30.2%\n",
            "41 loss train: 3.4461\t val 0.2041\tAcc (val): 30.5%\n",
            "42 loss train: 3.5741\t val 0.2042\tAcc (val): 30.6%\n",
            "43 loss train: 3.3093\t val 0.2041\tAcc (val): 30.9%\n",
            "44 loss train: 3.0680\t val 0.2040\tAcc (val): 31.2%\n",
            "45 loss train: 3.3371\t val 0.2039\tAcc (val): 30.7%\n",
            "46 loss train: 3.5336\t val 0.2038\tAcc (val): 30.6%\n",
            "47 loss train: 3.3410\t val 0.2038\tAcc (val): 30.4%\n",
            "48 loss train: 3.2315\t val 0.2039\tAcc (val): 30.7%\n",
            "49 loss train: 3.5350\t val 0.2038\tAcc (val): 30.6%\n",
            "50 loss train: 3.7756\t val 0.2037\tAcc (val): 30.5%\n",
            "51 loss train: 3.2359\t val 0.2036\tAcc (val): 29.8%\n",
            "52 loss train: 3.1459\t val 0.2033\tAcc (val): 30.4%\n",
            "53 loss train: 3.1553\t val 0.2032\tAcc (val): 29.9%\n",
            "54 loss train: 3.3957\t val 0.2033\tAcc (val): 29.7%\n",
            "55 loss train: 3.4838\t val 0.2031\tAcc (val): 29.9%\n",
            "56 loss train: 3.4696\t val 0.2031\tAcc (val): 29.7%\n",
            "57 loss train: 3.5766\t val 0.2030\tAcc (val): 29.7%\n",
            "58 loss train: 3.4628\t val 0.2029\tAcc (val): 29.6%\n",
            "59 loss train: 3.2668\t val 0.2028\tAcc (val): 29.7%\n",
            "60 loss train: 3.7154\t val 0.2029\tAcc (val): 29.6%\n",
            "61 loss train: 3.1964\t val 0.2028\tAcc (val): 29.7%\n",
            "62 loss train: 3.0964\t val 0.2027\tAcc (val): 30.1%\n",
            "63 loss train: 3.4385\t val 0.2026\tAcc (val): 30.3%\n",
            "64 loss train: 3.1455\t val 0.2025\tAcc (val): 30.7%\n",
            "65 loss train: 3.3665\t val 0.2024\tAcc (val): 30.7%\n",
            "66 loss train: 3.5691\t val 0.2025\tAcc (val): 30.9%\n",
            "67 loss train: 3.0517\t val 0.2024\tAcc (val): 30.9%\n",
            "68 loss train: 3.3166\t val 0.2021\tAcc (val): 30.6%\n",
            "69 loss train: 3.2280\t val 0.2020\tAcc (val): 30.3%\n",
            "70 loss train: 3.1421\t val 0.2020\tAcc (val): 30.6%\n",
            "71 loss train: 3.1576\t val 0.2016\tAcc (val): 30.6%\n",
            "72 loss train: 3.5146\t val 0.2015\tAcc (val): 30.0%\n",
            "73 loss train: 3.2848\t val 0.2013\tAcc (val): 30.0%\n",
            "74 loss train: 3.2359\t val 0.2009\tAcc (val): 30.8%\n",
            "75 loss train: 3.6027\t val 0.2009\tAcc (val): 30.6%\n",
            "76 loss train: 3.5798\t val 0.2010\tAcc (val): 30.5%\n",
            "77 loss train: 3.4808\t val 0.2006\tAcc (val): 30.3%\n",
            "78 loss train: 3.2790\t val 0.2004\tAcc (val): 30.4%\n",
            "79 loss train: 3.4717\t val 0.2001\tAcc (val): 30.5%\n",
            "80 loss train: 3.3734\t val 0.2000\tAcc (val): 30.4%\n",
            "81 loss train: 3.5396\t val 0.1996\tAcc (val): 30.6%\n",
            "82 loss train: 3.1462\t val 0.1992\tAcc (val): 30.7%\n",
            "83 loss train: 3.3454\t val 0.1990\tAcc (val): 31.2%\n",
            "84 loss train: 3.0271\t val 0.1988\tAcc (val): 31.4%\n",
            "85 loss train: 3.4356\t val 0.1985\tAcc (val): 31.4%\n",
            "86 loss train: 3.4264\t val 0.1983\tAcc (val): 31.2%\n",
            "87 loss train: 3.3516\t val 0.1981\tAcc (val): 31.5%\n",
            "88 loss train: 3.2354\t val 0.1980\tAcc (val): 31.5%\n",
            "89 loss train: 3.6373\t val 0.1981\tAcc (val): 31.3%\n",
            "90 loss train: 3.3151\t val 0.1982\tAcc (val): 31.2%\n",
            "91 loss train: 3.5358\t val 0.1981\tAcc (val): 31.7%\n",
            "92 loss train: 3.2835\t val 0.1979\tAcc (val): 31.2%\n",
            "93 loss train: 3.4921\t val 0.1979\tAcc (val): 31.7%\n",
            "94 loss train: 3.3327\t val 0.1980\tAcc (val): 31.2%\n",
            "95 loss train: 3.5170\t val 0.1980\tAcc (val): 31.2%\n",
            "96 loss train: 3.1509\t val 0.1980\tAcc (val): 31.5%\n",
            "97 loss train: 3.1339\t val 0.1980\tAcc (val): 31.7%\n",
            "98 loss train: 3.1008\t val 0.1978\tAcc (val): 31.8%\n",
            "99 loss train: 3.5811\t val 0.1977\tAcc (val): 31.8%\n",
            "100 loss train: 3.5489\t val 0.1977\tAcc (val): 31.5%\n",
            "101 loss train: 3.3432\t val 0.1976\tAcc (val): 31.8%\n",
            "102 loss train: 3.4717\t val 0.1974\tAcc (val): 31.4%\n",
            "103 loss train: 3.4167\t val 0.1974\tAcc (val): 31.5%\n",
            "104 loss train: 3.6367\t val 0.1974\tAcc (val): 31.6%\n",
            "105 loss train: 3.6157\t val 0.1974\tAcc (val): 31.2%\n",
            "106 loss train: 3.5117\t val 0.1975\tAcc (val): 31.2%\n",
            "107 loss train: 3.3763\t val 0.1974\tAcc (val): 31.8%\n",
            "108 loss train: 3.4943\t val 0.1971\tAcc (val): 31.8%\n",
            "109 loss train: 3.2802\t val 0.1972\tAcc (val): 31.9%\n",
            "110 loss train: 3.5250\t val 0.1969\tAcc (val): 31.5%\n",
            "111 loss train: 3.3199\t val 0.1964\tAcc (val): 31.8%\n",
            "112 loss train: 3.3599\t val 0.1963\tAcc (val): 31.7%\n",
            "113 loss train: 3.2426\t val 0.1961\tAcc (val): 32.1%\n",
            "114 loss train: 3.1958\t val 0.1961\tAcc (val): 32.4%\n",
            "115 loss train: 3.2119\t val 0.1958\tAcc (val): 32.8%\n",
            "116 loss train: 3.2928\t val 0.1960\tAcc (val): 32.9%\n",
            "117 loss train: 3.0748\t val 0.1957\tAcc (val): 32.9%\n",
            "118 loss train: 3.3844\t val 0.1954\tAcc (val): 33.1%\n",
            "119 loss train: 3.4656\t val 0.1956\tAcc (val): 33.4%\n",
            "120 loss train: 3.5237\t val 0.1957\tAcc (val): 33.4%\n",
            "121 loss train: 3.1881\t val 0.1956\tAcc (val): 33.4%\n",
            "122 loss train: 3.4911\t val 0.1954\tAcc (val): 33.9%\n",
            "123 loss train: 3.4571\t val 0.1951\tAcc (val): 33.6%\n",
            "124 loss train: 3.4310\t val 0.1950\tAcc (val): 34.2%\n",
            "125 loss train: 3.7132\t val 0.1949\tAcc (val): 34.0%\n",
            "126 loss train: 3.2293\t val 0.1948\tAcc (val): 34.7%\n",
            "127 loss train: 3.1772\t val 0.1947\tAcc (val): 34.9%\n",
            "128 loss train: 3.2558\t val 0.1947\tAcc (val): 35.0%\n",
            "129 loss train: 3.3224\t val 0.1944\tAcc (val): 34.9%\n",
            "130 loss train: 3.4249\t val 0.1943\tAcc (val): 34.3%\n",
            "131 loss train: 3.6359\t val 0.1944\tAcc (val): 34.0%\n",
            "132 loss train: 3.3420\t val 0.1941\tAcc (val): 33.2%\n",
            "133 loss train: 3.4175\t val 0.1939\tAcc (val): 33.7%\n",
            "134 loss train: 3.6002\t val 0.1940\tAcc (val): 33.4%\n",
            "135 loss train: 3.5158\t val 0.1941\tAcc (val): 34.3%\n",
            "136 loss train: 3.3246\t val 0.1938\tAcc (val): 33.8%\n",
            "137 loss train: 3.5273\t val 0.1937\tAcc (val): 34.1%\n",
            "138 loss train: 3.4848\t val 0.1936\tAcc (val): 34.0%\n",
            "139 loss train: 3.1233\t val 0.1932\tAcc (val): 34.3%\n",
            "140 loss train: 3.4472\t val 0.1933\tAcc (val): 34.2%\n",
            "141 loss train: 3.5260\t val 0.1932\tAcc (val): 33.9%\n",
            "142 loss train: 3.4480\t val 0.1933\tAcc (val): 34.0%\n",
            "143 loss train: 3.6271\t val 0.1934\tAcc (val): 34.1%\n",
            "144 loss train: 3.1921\t val 0.1934\tAcc (val): 33.7%\n",
            "145 loss train: 3.3478\t val 0.1932\tAcc (val): 34.0%\n",
            "146 loss train: 3.3432\t val 0.1932\tAcc (val): 34.0%\n",
            "147 loss train: 3.4230\t val 0.1933\tAcc (val): 34.6%\n",
            "148 loss train: 3.1354\t val 0.1933\tAcc (val): 34.6%\n",
            "149 loss train: 3.2783\t val 0.1930\tAcc (val): 34.9%\n",
            "150 loss train: 3.2938\t val 0.1929\tAcc (val): 34.9%\n",
            "151 loss train: 3.3138\t val 0.1930\tAcc (val): 35.2%\n",
            "152 loss train: 3.4906\t val 0.1928\tAcc (val): 35.6%\n",
            "153 loss train: 3.0994\t val 0.1922\tAcc (val): 36.0%\n",
            "154 loss train: 3.7184\t val 0.1921\tAcc (val): 35.7%\n",
            "155 loss train: 2.9482\t val 0.1923\tAcc (val): 35.7%\n",
            "156 loss train: 3.5340\t val 0.1920\tAcc (val): 35.7%\n",
            "157 loss train: 3.6112\t val 0.1920\tAcc (val): 36.0%\n",
            "158 loss train: 3.4632\t val 0.1916\tAcc (val): 36.2%\n",
            "159 loss train: 3.3499\t val 0.1917\tAcc (val): 36.8%\n",
            "160 loss train: 3.3607\t val 0.1912\tAcc (val): 36.9%\n",
            "161 loss train: 3.3685\t val 0.1911\tAcc (val): 36.7%\n",
            "162 loss train: 3.4480\t val 0.1909\tAcc (val): 37.3%\n",
            "163 loss train: 3.2555\t val 0.1910\tAcc (val): 37.5%\n",
            "164 loss train: 3.1443\t val 0.1909\tAcc (val): 37.6%\n",
            "165 loss train: 3.9745\t val 0.1898\tAcc (val): 37.6%\n",
            "EPOCH 4 / 29\n",
            "0 loss train: 2.9974\t val 0.1895\tAcc (val): 38.0%\n",
            "1 loss train: 3.2319\t val 0.1895\tAcc (val): 38.1%\n",
            "2 loss train: 3.1101\t val 0.1894\tAcc (val): 38.1%\n",
            "3 loss train: 3.5389\t val 0.1893\tAcc (val): 37.9%\n",
            "4 loss train: 3.1961\t val 0.1893\tAcc (val): 37.7%\n",
            "5 loss train: 2.9200\t val 0.1893\tAcc (val): 37.7%\n",
            "6 loss train: 3.1380\t val 0.1893\tAcc (val): 37.5%\n",
            "7 loss train: 3.7864\t val 0.1894\tAcc (val): 37.7%\n",
            "8 loss train: 3.0747\t val 0.1895\tAcc (val): 37.4%\n",
            "9 loss train: 3.3724\t val 0.1895\tAcc (val): 37.6%\n",
            "10 loss train: 2.9627\t val 0.1895\tAcc (val): 37.4%\n",
            "11 loss train: 3.2183\t val 0.1894\tAcc (val): 37.5%\n",
            "12 loss train: 3.1160\t val 0.1893\tAcc (val): 37.4%\n",
            "13 loss train: 3.6520\t val 0.1894\tAcc (val): 37.5%\n",
            "14 loss train: 3.0365\t val 0.1893\tAcc (val): 37.6%\n",
            "15 loss train: 3.0648\t val 0.1891\tAcc (val): 37.7%\n",
            "16 loss train: 3.1395\t val 0.1892\tAcc (val): 37.7%\n",
            "17 loss train: 3.6916\t val 0.1891\tAcc (val): 37.7%\n",
            "18 loss train: 3.2715\t val 0.1895\tAcc (val): 37.4%\n",
            "19 loss train: 2.8996\t val 0.1893\tAcc (val): 37.2%\n",
            "20 loss train: 3.2369\t val 0.1893\tAcc (val): 37.3%\n",
            "21 loss train: 3.0390\t val 0.1895\tAcc (val): 37.0%\n",
            "22 loss train: 2.9724\t val 0.1891\tAcc (val): 36.8%\n",
            "23 loss train: 3.2323\t val 0.1889\tAcc (val): 37.0%\n",
            "24 loss train: 3.2224\t val 0.1886\tAcc (val): 36.5%\n",
            "25 loss train: 3.2990\t val 0.1885\tAcc (val): 36.5%\n",
            "26 loss train: 3.3197\t val 0.1886\tAcc (val): 36.2%\n",
            "27 loss train: 3.1749\t val 0.1882\tAcc (val): 36.3%\n",
            "28 loss train: 2.9906\t val 0.1882\tAcc (val): 36.5%\n",
            "29 loss train: 3.0761\t val 0.1878\tAcc (val): 36.9%\n",
            "30 loss train: 3.1724\t val 0.1876\tAcc (val): 36.6%\n",
            "31 loss train: 3.2034\t val 0.1875\tAcc (val): 36.4%\n",
            "32 loss train: 3.2172\t val 0.1875\tAcc (val): 36.3%\n",
            "33 loss train: 3.1820\t val 0.1874\tAcc (val): 36.2%\n",
            "34 loss train: 3.1353\t val 0.1875\tAcc (val): 36.5%\n",
            "35 loss train: 2.8883\t val 0.1873\tAcc (val): 36.7%\n",
            "36 loss train: 3.2006\t val 0.1872\tAcc (val): 36.8%\n",
            "37 loss train: 3.3768\t val 0.1872\tAcc (val): 36.8%\n",
            "38 loss train: 3.1875\t val 0.1870\tAcc (val): 37.0%\n",
            "39 loss train: 3.1416\t val 0.1869\tAcc (val): 37.7%\n",
            "40 loss train: 2.9390\t val 0.1870\tAcc (val): 37.4%\n",
            "41 loss train: 3.1487\t val 0.1869\tAcc (val): 37.0%\n",
            "42 loss train: 3.1783\t val 0.1865\tAcc (val): 36.7%\n",
            "43 loss train: 3.2467\t val 0.1865\tAcc (val): 36.6%\n",
            "44 loss train: 3.2586\t val 0.1864\tAcc (val): 36.9%\n",
            "45 loss train: 2.8325\t val 0.1863\tAcc (val): 37.8%\n",
            "46 loss train: 3.2278\t val 0.1861\tAcc (val): 37.4%\n",
            "47 loss train: 3.2215\t val 0.1860\tAcc (val): 37.7%\n",
            "48 loss train: 3.3152\t val 0.1859\tAcc (val): 38.0%\n",
            "49 loss train: 3.2882\t val 0.1857\tAcc (val): 38.3%\n",
            "50 loss train: 3.1446\t val 0.1856\tAcc (val): 38.4%\n",
            "51 loss train: 3.2258\t val 0.1855\tAcc (val): 39.0%\n",
            "52 loss train: 3.1656\t val 0.1856\tAcc (val): 38.8%\n",
            "53 loss train: 3.3147\t val 0.1857\tAcc (val): 39.0%\n",
            "54 loss train: 3.5270\t val 0.1859\tAcc (val): 38.8%\n",
            "55 loss train: 3.0807\t val 0.1857\tAcc (val): 38.9%\n",
            "56 loss train: 3.2930\t val 0.1853\tAcc (val): 38.8%\n",
            "57 loss train: 3.2862\t val 0.1850\tAcc (val): 38.7%\n",
            "58 loss train: 3.0272\t val 0.1848\tAcc (val): 38.7%\n",
            "59 loss train: 2.9428\t val 0.1844\tAcc (val): 38.4%\n",
            "60 loss train: 3.0169\t val 0.1842\tAcc (val): 38.7%\n",
            "61 loss train: 3.2227\t val 0.1838\tAcc (val): 38.7%\n",
            "62 loss train: 2.8482\t val 0.1837\tAcc (val): 38.5%\n",
            "63 loss train: 2.8456\t val 0.1832\tAcc (val): 38.4%\n",
            "64 loss train: 3.1838\t val 0.1829\tAcc (val): 38.1%\n",
            "65 loss train: 3.4359\t val 0.1828\tAcc (val): 38.4%\n",
            "66 loss train: 3.1376\t val 0.1826\tAcc (val): 38.4%\n",
            "67 loss train: 3.2109\t val 0.1827\tAcc (val): 38.0%\n",
            "68 loss train: 3.1186\t val 0.1830\tAcc (val): 37.4%\n",
            "69 loss train: 3.2549\t val 0.1831\tAcc (val): 37.9%\n",
            "70 loss train: 3.3483\t val 0.1835\tAcc (val): 38.3%\n",
            "71 loss train: 3.0089\t val 0.1837\tAcc (val): 38.3%\n",
            "72 loss train: 3.3566\t val 0.1838\tAcc (val): 38.4%\n",
            "73 loss train: 3.1311\t val 0.1836\tAcc (val): 38.7%\n",
            "74 loss train: 3.1575\t val 0.1837\tAcc (val): 38.2%\n",
            "75 loss train: 3.1102\t val 0.1834\tAcc (val): 37.7%\n",
            "76 loss train: 2.9811\t val 0.1831\tAcc (val): 37.8%\n",
            "77 loss train: 3.4923\t val 0.1836\tAcc (val): 38.0%\n",
            "78 loss train: 3.0708\t val 0.1837\tAcc (val): 37.7%\n",
            "79 loss train: 2.9470\t val 0.1836\tAcc (val): 37.4%\n",
            "80 loss train: 3.0354\t val 0.1837\tAcc (val): 37.5%\n",
            "81 loss train: 3.0858\t val 0.1838\tAcc (val): 36.8%\n",
            "82 loss train: 2.9675\t val 0.1836\tAcc (val): 37.1%\n",
            "83 loss train: 3.2756\t val 0.1832\tAcc (val): 37.5%\n",
            "84 loss train: 2.8448\t val 0.1835\tAcc (val): 36.9%\n",
            "85 loss train: 3.2016\t val 0.1834\tAcc (val): 36.9%\n",
            "86 loss train: 3.4852\t val 0.1836\tAcc (val): 36.9%\n",
            "87 loss train: 2.8590\t val 0.1835\tAcc (val): 36.8%\n",
            "88 loss train: 2.9475\t val 0.1835\tAcc (val): 37.1%\n",
            "89 loss train: 2.8457\t val 0.1832\tAcc (val): 37.0%\n",
            "90 loss train: 2.8262\t val 0.1830\tAcc (val): 37.4%\n",
            "91 loss train: 3.0513\t val 0.1828\tAcc (val): 37.4%\n",
            "92 loss train: 3.3489\t val 0.1826\tAcc (val): 36.8%\n",
            "93 loss train: 3.1216\t val 0.1826\tAcc (val): 37.4%\n",
            "94 loss train: 3.1583\t val 0.1828\tAcc (val): 37.8%\n",
            "95 loss train: 3.2999\t val 0.1822\tAcc (val): 38.0%\n",
            "96 loss train: 3.4240\t val 0.1820\tAcc (val): 38.0%\n",
            "97 loss train: 2.8139\t val 0.1819\tAcc (val): 39.0%\n",
            "98 loss train: 3.0097\t val 0.1815\tAcc (val): 38.9%\n",
            "99 loss train: 3.1408\t val 0.1818\tAcc (val): 39.4%\n",
            "100 loss train: 3.0517\t val 0.1816\tAcc (val): 39.9%\n",
            "101 loss train: 2.9879\t val 0.1817\tAcc (val): 40.0%\n",
            "102 loss train: 3.0455\t val 0.1814\tAcc (val): 40.1%\n",
            "103 loss train: 2.9011\t val 0.1814\tAcc (val): 40.1%\n",
            "104 loss train: 3.4206\t val 0.1813\tAcc (val): 40.8%\n",
            "105 loss train: 3.0467\t val 0.1816\tAcc (val): 40.2%\n",
            "106 loss train: 3.0608\t val 0.1817\tAcc (val): 40.4%\n",
            "107 loss train: 3.2264\t val 0.1816\tAcc (val): 40.4%\n",
            "108 loss train: 3.2312\t val 0.1817\tAcc (val): 39.8%\n",
            "109 loss train: 3.2160\t val 0.1813\tAcc (val): 39.9%\n",
            "110 loss train: 3.1130\t val 0.1814\tAcc (val): 39.9%\n",
            "111 loss train: 3.2387\t val 0.1813\tAcc (val): 39.9%\n",
            "112 loss train: 3.3593\t val 0.1815\tAcc (val): 39.3%\n",
            "113 loss train: 3.1768\t val 0.1816\tAcc (val): 39.3%\n",
            "114 loss train: 2.8200\t val 0.1814\tAcc (val): 39.3%\n",
            "115 loss train: 2.9775\t val 0.1808\tAcc (val): 39.4%\n",
            "116 loss train: 3.1221\t val 0.1807\tAcc (val): 39.6%\n",
            "117 loss train: 2.9352\t val 0.1805\tAcc (val): 39.9%\n",
            "118 loss train: 3.1215\t val 0.1801\tAcc (val): 39.7%\n",
            "119 loss train: 3.2131\t val 0.1799\tAcc (val): 40.0%\n",
            "120 loss train: 3.3390\t val 0.1802\tAcc (val): 39.8%\n",
            "121 loss train: 2.9752\t val 0.1801\tAcc (val): 40.1%\n",
            "122 loss train: 3.0234\t val 0.1802\tAcc (val): 39.8%\n",
            "123 loss train: 3.3024\t val 0.1800\tAcc (val): 39.9%\n",
            "124 loss train: 3.1311\t val 0.1802\tAcc (val): 40.1%\n",
            "125 loss train: 3.0572\t val 0.1804\tAcc (val): 40.2%\n",
            "126 loss train: 2.9704\t val 0.1802\tAcc (val): 40.5%\n",
            "127 loss train: 2.9076\t val 0.1799\tAcc (val): 40.5%\n",
            "128 loss train: 2.9151\t val 0.1796\tAcc (val): 40.3%\n",
            "129 loss train: 2.9422\t val 0.1797\tAcc (val): 40.4%\n",
            "130 loss train: 3.5072\t val 0.1799\tAcc (val): 40.3%\n",
            "131 loss train: 2.8373\t val 0.1801\tAcc (val): 40.4%\n"
          ]
        }
      ],
      "source": [
        "# Récupérer un réseau pré-entraîné (resnet-18)\n",
        "print(\"Récupération du ResNet-18 pré-entraîné...\")\n",
        "my_net = models.resnet18(pretrained=True)\n",
        "#my_net = models.mobilenet_v3_small(pretrained=True) #pour mobilenet\n",
        "\n",
        "# on indique qu'il est inutile de calculer les gradients des paramètres du réseau\n",
        "for param in my_net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "my_net.fc = nn.Linear(in_features=my_net.fc.in_features, out_features=nb_classes, bias=True)\n",
        "\n",
        "my_net.to(device) \n",
        "my_net.train(True) \n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(my_net.fc.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
        "\n",
        "print(\"Apprentissage en transfer learning\")\n",
        "my_net.train(True)\n",
        "torch.manual_seed(42)\n",
        "losses, accuracies = train_model(my_net, loader_train, dataset_val, optimizer, criterion, n_epochs=NB_EPOCHS)\n",
        "# évaluation\n",
        "my_net.train(False)\n",
        "loss, accuracy = evaluate(my_net, dataset_test)\n",
        "print(\"Accuracy (test): %.1f%%\" % (100 * accuracy))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot accuracy and loss\n",
        "x = range(NB_EPOCHS)\n",
        "plt.figure(figsize=(20,8))\n",
        "ax1 = plt.subplot(121)\n",
        "ax1.set_title(\"Loss\")\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "plt.plot(x, losses)\n",
        "ax2 = plt.subplot(122)\n",
        "ax2.set_title(\"Accuracy\")\n",
        "plt.plot(x, accuracies)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "jYLZJfi8HMi6",
        "outputId": "11ad3d5a-9121-42a3-b812-95e41a05820d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1be71b8f0d92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#plot accuracy and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNB_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m121\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'NB_EPOCHS' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eguF4OG6lAIV"
      },
      "outputs": [],
      "source": [
        "#===== Fine tuning =====\n",
        "\n",
        "# on réinitialise resnet\n",
        "my_net = models.resnet18(pretrained=True)\n",
        "my_net.fc = nn.Linear(in_features=my_net.fc.in_features, out_features=nb_classes, bias=True)\n",
        "my_net.to(device)\n",
        "\n",
        "# cette fois on veut updater tous les paramètres\n",
        "params_to_update = my_net.parameters()\n",
        "\n",
        "# il est possible de ne sélectionner que quelques couches\n",
        "#     (plutôt parmi les \"dernières\", proches de la loss)\n",
        "#    Exemple (dans ce cas, oter \"params_to_update = my_net.parameters()\") ci-dessus\n",
        "# list_of_layers_to_finetune=['fc.weight','fc.bias','layer4.1.conv2.weight','layer4.1.bn2.bias','layer4.1.bn2.weight']\n",
        "# params_to_update=[]\n",
        "# for name,param in my_net.named_parameters():\n",
        "#     if name in list_of_layers_to_finetune:\n",
        "#         print(\"fine tune \",name)\n",
        "#         params_to_update.append(param)\n",
        "#         param.requires_grad = True\n",
        "#     else:\n",
        "#         param.requires_grad = False\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
        "\n",
        "# on ré-entraîne\n",
        "print(\"Apprentissage avec fine-tuning\")\n",
        "my_net.train(True)\n",
        "torch.manual_seed(42)\n",
        "train_model(my_net, loader_train, dataset_val, optimizer, criterion, n_epochs=10)\n",
        "\n",
        "# on ré-évalue les performances\n",
        "my_net.train(False)\n",
        "loss, accuracy = evaluate(my_net, dataset_test)\n",
        "print(\"Accuracy (test): %.1f%%\" % (100 * accuracy))\n"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}